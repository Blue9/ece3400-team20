<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta name="description" content="">
    <meta name="author" content="">
    <title>ECE 3400 Team 20 - Home</title>
    <!-- Bootstrap core CSS -->
    <link href="vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">

    <!-- Custom fonts for this template -->
    <link href="vendor/font-awesome/css/font-awesome.min.css" rel="stylesheet" type="text/css">
    <link href="https://fonts.googleapis.com/css?family=Montserrat:400,700" rel="stylesheet" type="text/css">
    <link href="https://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic" rel="stylesheet" type="text/css">
    <!-- Plugin CSS -->
    <link href="vendor/magnific-popup/magnific-popup.css" rel="stylesheet" type="text/css">
    <!-- Custom styles for this template -->
    <link href="css/freelancer.css" rel="stylesheet">
    <link rel="stylesheet" href="css/styles/default.css">
    <script src="js/highlight.pack.js"></script>
    <script>hljs.initHighlightingOnLoad();</script>
</head>

<body id="page-top">
    <!-- Navigation -->
    <style>
        img {
            max-width: 100%;
            text-align: center;
        }
        .mfp-content {
            max-width: 1200px;
        }
    </style>
    <nav class="navbar navbar-expand-lg bg-secondary fixed-top text-uppercase" id="mainNav">
        <div class="container">
            <a class="navbar-brand js-scroll-trigger" href="#page-top">ECE 3400</a>
            <button class="navbar-toggler navbar-toggler-right text-uppercase bg-primary text-white rounded" type="button"
                data-toggle="collapse" data-target="#navbarResponsive" aria-controls="navbarResponsive" aria-expanded="false"
                aria-label="Toggle navigation">
                Menu
                <i class="fa fa-bars"></i>
            </button>
            <div class="collapse navbar-collapse" id="navbarResponsive">
                <ul class="navbar-nav ml-auto">
                    <li class="nav-item mx-0 mx-lg-1">
                        <a class="nav-link py-3 px-0 px-lg-3 rounded active" href="index.html">Labs</a>
                    </li>
                    <li class="nav-item mx-0 mx-lg-1">
                        <a class="nav-link py-3 px-0 px-lg-3 rounded" href="teamcontract.html">Contract</a>
                    </li>
                    <li class="nav-item mx-0 mx-lg-1">
                        <a class="nav-link py-3 px-0 px-lg-3 rounded" href="ethics.html">Ethics HW</a>
                    </li>
                    <li class="nav-item mx-0 mx-lg-1">
                        <a class="nav-link py-3 px-0 px-lg-3 rounded" href="final.html">Design Report</a>
                      </li>
                    <li class="nav-item mx-0 mx-lg-1">
                        <a class="nav-link py-3 px-0 px-lg-3 rounded" href="https://github.com/Blue9/ece3400-team20">GitHub
                            <i class="fa fa-external-link"></i></a>
                    </li>
                </ul>
            </div>
        </div>
    </nav>
    <!-- Header -->
    <header class="masthead bg-primary text-white text-center">
        <div class="container">
            <div class="w-100 text-center mb-4">
                <div class="shiny" style="display: inline-block; width:256px; height: 256px; border-radius: 128px;">
                    <img class="img-fluid mb-5 d-block mx-auto" src="img/Circuit_Bear.png" alt="" style="width: 256px; height: 256px;">
                </div>
            </div>
            <h1 class="text-uppercase mb-0">The Omega</h1>
            <hr class="star-light">
        </div>

    </header>
    <!-- Portfolio Grid Section -->
    <section class="portfolio" id="portfolio">
        <div class="container">
            <h2 class="text-center text-uppercase text-secondary mb-0">Labs</h2>
            <hr class="star-dark mb-5">
            <div class="row">
                <div class="col-md-6 col-lg-3">
                    <a class="portfolio-item d-block mx-auto" href="#portfolio-modal-1">
                        <div class="portfolio-item-caption d-flex position-absolute h-100 w-100">
                            <div class="portfolio-item-caption-content my-auto w-100 text-center text-white">
                                <h3>Lab 1</h3>
                            </div>
                        </div>
                        <img class="img-fluid" src="img/portfolio/arduino.png" alt="">
                    </a>
                </div>
                <div class="col-md-6 col-lg-3">
                    <a class="portfolio-item d-block mx-auto" href="#portfolio-modal-2">
                        <div class="portfolio-item-caption d-flex position-absolute h-100 w-100">
                            <div class="portfolio-item-caption-content my-auto w-100 text-center text-white">
                                <h3>Lab 2</h3>
                            </div>
                        </div>
                        <img class="img-fluid" src="img/portfolio/Mic_LED.png" alt="">
                    </a>
                </div>
                <div class="col-md-6 col-lg-3">
                    <a class="portfolio-item d-block mx-auto" href="#portfolio-modal-3">
                        <div class="portfolio-item-caption d-flex position-absolute h-100 w-100">
                            <div class="portfolio-item-caption-content my-auto w-100 text-center text-white">
                                <h3>Lab 3</h3>
                            </div>
                        </div>
                        <img class="img-fluid" src="img/portfolio/transmitter.png" alt="">
                    </a>
                </div>
                <div class="col-md-6 col-lg-3">
                    <a class="portfolio-item d-block mx-auto" href="#portfolio-modal-4">
                        <div class="portfolio-item-caption d-flex position-absolute h-100 w-100">
                            <div class="portfolio-item-caption-content my-auto w-100 text-center text-white">
                                <h3>Lab 4</h3>
                            </div>
                        </div>
                        <img class="img-fluid" src="img/portfolio/Lens.png" alt="">
                    </a>
                </div>
            </div>
            <h2 class="text-center text-uppercase text-secondary mb-0">Milestones</h2>
            <hr class="star-dark mb-5">
            <div class="row">
                <div class="col-md-6 col-lg-3">
                    <a class="portfolio-item d-block mx-auto " href="#portfolio-modal-5">
                        <div class="portfolio-item-caption d-flex position-absolute h-100 w-100">
                            <div class="portfolio-item-caption-content my-auto w-100 text-center text-white">
                                <h3> Milestone 1</h3>
                            </div>
                        </div>
                        <img class="img-fluid" src="img/portfolio/Seven_segment_milestone.png" alt="">
                    </a>
                </div>
                <div class="col-md-6 col-lg-3">
                    <a class="portfolio-item d-block mx-auto " href="#portfolio-modal-6">
                        <div class="portfolio-item-caption d-flex position-absolute h-100 w-100">
                            <div class="portfolio-item-caption-content my-auto w-100 text-center text-white">
                                <h3>Milestone 2</h3>
                            </div>
                        </div>
                        <img class="img-fluid" src="img/portfolio/maze_milestone.png" alt="">
                    </a>
                </div>
                <div class="col-md-6 col-lg-3">
                    <a class="portfolio-item d-block mx-auto" href="#portfolio-modal-7">
                        <div class="portfolio-item-caption d-flex position-absolute h-100 w-100">
                            <div class="portfolio-item-caption-content my-auto w-100 text-center text-white">
                                <h3>Milestone 3</h3>
                            </div>
                        </div>
                        <img class="img-fluid" src="img/portfolio/arduino.png" alt="">
                    </a>
                </div>
                <div class="col-md-6 col-lg-3">
                    <a class="portfolio-item d-block mx-auto" href="#portfolio-modal-8">
                        <div class="portfolio-item-caption d-flex position-absolute h-100 w-100">
                            <div class="portfolio-item-caption-content my-auto w-100 text-center text-white">
                                <h3>Milestone 4</h3>
                            </div>
                        </div>
                        <img class="img-fluid" src="img/portfolio/treasure_milestone.png" alt="">
                    </a>
                </div>
            </div>
        </div>
    </section>
    <!-- About Section
            <section class="bg-primary text-white mb-0" id="about">
            <div class="container">
            <h2 class="text-center text-uppercase text-white">About</h2>
            <hr class="star-light mb-5">
            <div class="row">
            <div class="col-lg-4 ml-auto">
            <p class="lead">Freelancer is a free bootstrap theme created by Start Bootstrap. The download includes the complete source files including HTML, CSS, and JavaScript as well as optional LESS stylesheets for easy customization.</p>
            </div>
            <div class="col-lg-4 mr-auto">
            <p class="lead">Whether you're a student looking to showcase your work, a professional looking to attract clients, or a graphic artist looking to share your projects, this template is the perfect starting point!</p>
            </div>
            </div>
            <div class="text-center mt-4">
            <a class="btn btn-xl btn-outline-light" href="#">
            <i class="fa fa-download mr-2"></i>
            Download Now!
            </a>
            </div>
            </div>
            </section> -->
    <!-- Footer -->
    <footer class="footer text-center">
        <div class="container">
            <div class="row">
                <div class="col-md-4 mb-5 mb-lg-0">
                    <h4 class="text-uppercase mb-4">Location</h4>
                    <p class="lead mb-0">Cornell University
                        <br>Ithaca, NY 14853
                    </p>
                </div>
                <div class="col-md-4 mb-5 mb-lg-0">
                    <h4 class="text-uppercase mb-4">Around the Web</h4>
                    <ul class="list-inline mb-0">
                        <li class="list-inline-item">
                            <a class="btn btn-outline-light btn-social text-center rounded-circle" href="#">
                                <i class="fa fa-fw fa-facebook"></i>
                            </a>
                        </li>
                        <li class="list-inline-item">
                            <a class="btn btn-outline-light btn-social text-center rounded-circle" href="#">
                                <i class="fa fa-fw fa-google-plus"></i>
                            </a>
                        </li>
                        <li class="list-inline-item">
                            <a class="btn btn-outline-light btn-social text-center rounded-circle" href="#">
                                <i class="fa fa-fw fa-twitter"></i>
                            </a>
                        </li>
                        <li class="list-inline-item">
                            <a class="btn btn-outline-light btn-social text-center rounded-circle" href="#">
                                <i class="fa fa-fw fa-linkedin"></i>
                            </a>
                        </li>
                        <li class="list-inline-item">
                            <a class="btn btn-outline-light btn-social text-center rounded-circle" href="#">
                                <i class="fa fa-fw fa-dribbble"></i>
                            </a>
                        </li>
                    </ul>
                </div>
                <div class="col-md-4">
                    <h4 class="text-uppercase mb-4">About Us</h4>
                    <p class="lead mb-0">We are an award-winning ECE 3400 team.</p>
                </div>
            </div>
        </div>
    </footer>
    <div class="copyright py-4 text-center text-white">
        <div class="container">
            <small>Copyright &copy; Team 20 2018</small>
        </div>
    </div>
    <!-- Scroll to Top Button (Only visible on small and extra-small screen sizes) -->
    <div class="scroll-to-top d-lg-none position-fixed ">
        <a class="js-scroll-trigger d-block text-center text-white rounded" href="#page-top">
            <i class="fa fa-chevron-up"></i>
        </a>
    </div>
    <!-- Portfolio Modals -->
    <!-- Portfolio Modal 1 -->
    <div class="portfolio-modal mfp-hide" id="portfolio-modal-1">
        <div class="portfolio-modal-dialog bg-white">
            <a class="close-button d-none d-md-block portfolio-modal-dismiss" href="#">
                <i class="fa fa-3x fa-times"></i>
            </a>
            <div class="container text-center">
                <div class="row">
                    <div class="col-lg-10 mx-auto">
                        <h2 class="text-secondary text-uppercase mb-0">Lab 1</h2>
                        <hr class="star-dark mb-5">
                        <div class="text-left">
                            <p><em>Objective</em>: The objective of this lab was to gain familiarity with the Arduino
                                UNO microcontroller and its IDE.</p>
                            <h3>Lab Procedure</h3>
                            <h4 class="heading">Communicating with the Arduino and the IDE</h4>
                            <p>We first opened the Arduino IDE and clicked under Files/Examples/01.Basic/Blink. In
                                order to make sure the Arduino UNO was properly connected to the IDE, we clicked under
                                Tools/Board/ and selected Arduino/Genuino UNO. We also verified that the IDE is
                                connected to the right port by looking under Tools/Port. It should be as in the
                                following images:</p>
                            <p class="text-center" style="max-width:100%; max-height: 400px;" src="https://raw.githubusercontent.com/Blue9/ece3400-team20/gh-pages/Labs/Blink.png?token=ALqGCLBrFr-mOmTYEvBasTaXfWTdYf-xks5blqK1wA%3D%3D"
                                    alt="ArduinoBlinkSketch"></p>
                            <p class="text-center"><img style="max-width:100%; max-height: 400px;" src="https://raw.githubusercontent.com/Blue9/ece3400-team20/gh-pages/Labs/Tools.png?token=ALqGCBe1R2TgNZlNnu1_rmeRQ2tHJ5pvks5blqNVwA%3D%3D"
                                    alt="ArduinoToolsImage"></p>
                            <p>We clicked the check mark to compile the sketch and uploaded it to the Arduino UNO. We
                                obtained the following result:</p>
                            <p class="text-center"><iframe width="560" height="315" src="https://www.youtube.com/embed/GjLLtRx1XvA"
                                frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></p>
                            <h4 class="heading">Modifying the Blink Sketch</h4>
                            <p>We modified the Blink sketch to communicate with an external LED powered by Pin 0 in
                                series with a 300 Ohm resistor on the breadboard. We also made sure to connect the
                                ground of the Arduino the ground of the LED. The wiring and operation is shown in the
                                following video and the modification to the code is as follows:</p>
                            <ul>
                                <li>Added <code>#define LED_PIN 0</code> at the top of the sketch</li>
                                <li>Replaced <code>LED_BUILTIN</code> with <code>LED_PIN</code></li>
                            </ul>
                            <p class="text-center"><iframe width="560" height="315" src="https://www.youtube.com/embed/NNcXywDYe_s"
                                frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></p>
                            <h4 class="heading">The Serial Monitor and the Analog Pins</h4>
                            <p>The first goal was to be able to see the output of the potentiometer on the Serial
                                monitor on the Arduino UNO. The potentiometer has three pins and so we connected one
                                end to ground, one end to the 5 Volt output on the Arduino Uno for power, and the
                                middle pin to an Analog Pin on the Arduino for reading. We used A0.</p>
                            <p>For the code, we set up the serial monitor and then used AnalogRead in order to read the
                                input. The code is as follows:</p>
                            <pre><code class="arduino">
<!--                         -->void setup() {
<!--                         -->    Serial.begin(9600); // setting up the Serial monitor
<!--                         -->}

<!--                         -->void loop() {
<!--                         -->    int a = analogRead(A0); // Read the Input
<!--                         -->    Serial.println(a); // Print the output to the serial monitor and create a new line
<!--                         -->}
                            </code></pre>
                            <p>Our result for this part is as follows:</p>
                            <p class="text-center"><iframe width="560" height="315" src="https://www.youtube.com/embed/0JiTJ-EnS_w"
                                frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></p>
                            <h4 class="heading">Analog Output</h4>
                            <p>The next goal was to be able to take the potentiometer value and map it to a PWM output
                                that goes to the LED. For reference on PWM, we found the following article helpful: <a
                                    href="https://en.wikipedia.org/wiki/Pulse-width_modulation">https://en.wikipedia.org/wiki/Pulse-width_modulation</a>.</p>
                            <p>We used the wiring from the previous part for the potentiometer and connected an LED in
                                series with a 300 Ohm resistor to digital pin 5 (PWM pin) all with common grounds.
                                <br> Our code for this was as follows:
                            </p>
                            <pre><code>
<!--                         -->#define POT_PIN A0
<!--                         -->#define LED_PIN 5

<!--                         -->// the setup function runs once when you press reset or power the board
<!--                         -->void setup() {
<!--                         -->    Serial.begin(9600);           // initialize the serial monitor
<!--                         -->    pinMode(LED_PIN, OUTPUT);     // setup the LED_PIN as output
<!--                         -->}

<!--                         -->// the loop function runs over and over again forever
<!--                         -->void loop() {
<!--                         -->    int a = analogRead(POT_PIN);  // read from the analog pin
<!--                         -->    analogWrite(LED_PIN, a/4);    // write to the PWM pin - scaling 1024 -&gt; 256
<!--                         -->}
                            </code></pre>
                            <p><em>Note:</em> <code>analogRead()</code> produces values from <code>0</code> to <code>1023</code>,
                                but <code>analogWrite()</code> only accepts values from <code>0</code> to <code>255</code>
                                so the input was scaled down by a factor of four.</p>
                            <p>What we observed was as follows:</p>
                            <p class="text-center"><iframe width="560" height="315" src="https://www.youtube.com/embed/IoDwbJaUMtU"
                                frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></p>
                            <h4 class="heading">Parallax Servos</h4>
                            <p>Instead of the output being written to an LED, we now wrote the output to a Parallax
                                Servos. The involved connecting the red wire to the 5 Volt Arduino output, the black
                                wire to ground, and the white wire to a PWM pin (Pin 5). The code for it is similar;
                                however involved now using the Servos.h library. And so we included this library at the
                                top of the sketch and read its documentation for setting up the Servo from the
                                following link: <a href="https://www.arduino.cc/en/Reference/Servo">https://www.arduino.cc/en/Reference/Servo</a>.</p>
                            <p>We used the following setup and code:</p>
                            <pre><code>
<!--                         -->#include &lt;Servo.h&gt;  // include the Servo library
<!--                         -->#define POT_PIN A0
<!--                         -->Servo myServo;  // create servo object to control a servo
<!--                         -->
<!--                         -->void setup() {
<!--                         -->    myServo.attach(9);            // attaches the servo on pin 9 to the servo object
<!--                         -->    Serial.begin(9600);           // initialize the serial monitor
<!--                         -->}
<!--                         -->
<!--                         -->void loop() {
<!--                         -->    int a = analogRead(POT_PIN);  // read from the analog pin
<!--                         -->    a = map(a, 0, 1023, 0, 180);  // scale 1024 to 180
<!--                         -->    myServo.write(a);             // sets the servo speed
<!--                         -->}
                            </code></pre>
                            <p>What we observed is as follows:</p>
                            <p class="text-center"><iframe width="560" height="315" src="https://www.youtube.com/embed/YBj1wgy29BY"
                                frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></p>
                            <h4 class="heading">Assembling the Robot</h4>
                            <p>We assembled the robot and wrote code involving writing to the Servos in order to make
                                an S shape. The following video
                                <br> demonstrates our fully autonomous moving robot:
                            </p>
                            <p class="text-center"><iframe width="560" height="315" src="https://www.youtube.com/embed/D5O05hFTHSU"
                                frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></p>
                        </div>
                        <a class="btn btn-primary btn-lg rounded-pill portfolio-modal-dismiss mt-4" href="#">
                            <i class="fa fa-close"></i> Close Project</a>
                    </div>
                </div>
            </div>
        </div>
    </div>
    <!-- Portfolio Modal 2 -->
    <div class="portfolio-modal mfp-hide" id="portfolio-modal-2">
        <div class="portfolio-modal-dialog bg-white">
            <a class="close-button d-none d-md-block portfolio-modal-dismiss" href="#">
                <i class="fa fa-3x fa-times"></i>
            </a>
            <div class="container text-center">
                <div class="row">
                    <div class="col-lg-10 mx-auto">
                        <h2 class="text-secondary text-uppercase mb-0">Lab 2</h2>
                        <hr class="star-dark mb-5">
                        <div class="text-left">
                            <p><em>Objective</em>: The goal of this lab was to design and implement analog and digital
                                circuitry to detect both optical and audio signals.</p>
                            <h2><a id="Lab_Procedure_4"></a>Lab Procedure</h2>
                            <h3><a id="Setup_6"></a>Setup</h3>
                            <p>The team was split into two groups, one to detect a 660 Hz audio tone that will signify
                                the
                                start of the competition, and one to detect a 6080 Hz infrared signal. Both groups
                                utilized
                                Open Music Labs Arduino FFT library to perform the required signal analysis. More
                                information on Open Music Labs can be found on their website <a href="http://wiki.openmusiclabs.com/wiki/ArduinoFFT">here</a>.</p>
                            <h3><a id="Audio_10"></a>Audio</h3>
                            <p>We started out by designing the amplification and filtering circuit.<br>
                                The fourier transform maps signals in the time domain into its constituent frequencies
                                in
                                the frequency domain. In a signal’s frequency representation, we can analyze the
                                magnitude
                                of various frequencies to see which frequencies are more present than others. Note that
                                the
                                fourier transform can be applied to both continuous and discrete signals. A fourier
                                transform applied to a discrete function is called the discrete fourier transform
                                (DFT). In
                                computer science, one algorithm to calculate the discrete fourier transform in an
                                optimized
                                way is the Fast Fourier Transform (FFT). The FFT samples a signal at varying time steps
                                and
                                then calculates its frequency components. In this lab we used the FFT to detect when a
                                660Hz tone is played.</p>
                            <p>The following formula can be used to calculate the DFT of a signal:</p>
                            <p class="text-center"><img style="max-width:100%; max-height: 400px;" src="https://raw.githubusercontent.com/Blue9/ece3400-team20/gh-pages/img/portfolio/DFT.png"
                                    alt="logo"></p>
                            <p>where x_k is a N-point sequence of complex points and X_k is the DFT vector.</p>
                            <p>We first brainstormed various amplifiers that would achieve a high enough gain to
                                amplify
                                the 660Hz signal. We decided to use an inverting Op Amp cascaded with a low pass
                                filter.</p>
                            <p>In the above schematic, the microphone input signal passes through DC signal blocking
                                capacitor C1, which is then fed into the inverting input of the op amp. The
                                non-inverting
                                input of the op amp is held at a DC bias of Vdd/2 for maximum output voltage swing. The
                                output signal from the op amp is then passed through a low pass filter with pole at
                                600kHz.
                                We did account for the fact that the LPF acts as a load of the Op Amp when we
                                experimented
                                with capacitor and resistor values. A rough calculation shows that the interstage
                                voltage
                                divider is not too significant. The final values we used amplified the signal producing
                                a
                                gain of around 100x and a low pass filter with cutoff frequency 4kHz. The below image
                                shows
                                an oscilloscope reading when a 660Hz tone is played around 10 cm away from the
                                microphone.</p>
                            <p class="text-center"><img style="max-width:100%; max-height: 400px;" src="https://raw.githubusercontent.com/Blue9/ece3400-team20/gh-pages/img/portfolio/Optical_Amplifier.jpg"
                                    alt="Oscilloscope reading"></p>
                            <p>The below image shows a frequency sweep of the operational amplifier. The cutoff
                                frequency
                                is around 4kHz as mentioned above. Note that this does not affect the gain of the 660Hz
                                signal.</p>
                            <p class="text-center"><img style="max-width:100%; max-height: 400px;" src="https://github.com/Blue9/ece3400-team20/blob/gh-pages/img/FilterFrequencySweep.png?raw=true"
                                    alt="Frequency sweep"></p>
                            <p>On the software side, we initially captured the FFT output from the analog circuit using
                                the
                                example code provided by the FFT library from the Open Music Lab: fft_adc_serial.ino.
                                This
                                script captures analog data on Arduino pin A0 using the embedded ADC at a sampling rate
                                of
                                approximately 38kHz. There are 256 samples (bins) per FFT taken, giving a resolution of
                                38,000/256 ≈ 150Hz. This indicates that we should see the 660Hz tone in bin
                                ceiling(660/150) = 5, which is what we observed. We wrote the following MATLAB script
                                to
                                capture the data output on to the serial port:</p>
                            <pre><code>
<!--                         -->% Ensure that COM port is open
<!--                         -->if ~isempty(instrfind)
<!--                         -->    fclose(instrfind);
<!--                         -->    delete(instrfind);
<!--                         -->end
<!--                         -->
<!--                         -->% Clear old data
<!--                         -->clear all
<!--                         -->close all
<!--                         -->
<!--                         -->% Open serial port on COM3
<!--                         -->myserialport = serial('COM3', 'BaudRate', 115200);
<!--                         -->fopen(myserialport);
<!--                         -->
<!--                         -->% Get data for approximately 4 FFTs to ensure that we get a complete one
<!--                         -->n = 1;
<!--                         -->while n &lt;= 512
<!--                         -->    % 'start' signal will generate matching failure warning
<!--                         -->    % Safe to ignore
<!--                         -->    temp = fscanf(myserialport,'%i');
<!--                         -->    if (isscalar(temp) &amp;&amp; isnumeric(temp))
<!--                         -->            y(n) = temp;
<!--                         -->            n = n+1;
<!--                         -->    end
<!--                         -->end
                            </code></pre>
                            <p>Using this code, we recorded the following data:</p>
                            <p>We observed that the 660Hz tone appeared as a strong peak in bin 5 as expected. In
                                addition,
                                tones at 1320Hz and 1960Hz appear in bins 10 and 15 respectively, demonstrating the
                                linearity of the FFT.<br>
                                Since the peak for 660Hz appears in such a low-numbered bin, we discovered that we
                                could
                                use a significantly slower sampling rate to conserve CPU time. The built-in Arduino
                                function AnalogRead is capable of this. It samples at approximately 8930Hz. This gives
                                us a
                                resolution of 8930/256 ≈ 35Hz. Using this method, we expect to see the 660Hz tone in
                                bin
                                ceiling(660/35) = 19. We modified the code as follows:</p>
                            <pre><code>
<!--                         -->#define LOG_OUT 1 // use the log output function
<!--                         -->#define FFT_N 256 // set to 256 point fft
<!--                         -->
<!--                         -->#include &lt;FFT.h&gt; // include the library
<!--                         -->
<!--                         -->void setup() {
<!--                         -->  Serial.begin(115200);
<!--                         -->}
<!--                         -->
<!--                         -->void loop() {
<!--                         -->  while(1) {
<!--                         -->    cli();
<!--                         -->    for (int i = 0; i &lt; 512 ; i +=2) {
<!--                         -->    fft_input[i] = analogRead(A0);
<!--                         -->    fft_input[i+1] = 0;
<!--                         -->    }
<!--                         -->    fft_window(); // window the data for better frequency response
<!--                         -->    fft_reorder(); // reorder the data before doing the fft
<!--                         -->    fft_run(); // process the data in the fft
<!--                         -->    fft_mag_log(); // take the output of the fft
<!--                         -->    sei();
<!--                         -->    for (byte i = 0 ; i &lt; FFT_N/2 ; i++) {
<!--                         -->        Serial.println(fft_log_out[i]); // send out the data
<!--                         -->    }
<!--                         -->  }
<!--                         -->}
                            </code></pre>
                            <p>Using the same MATLAB script, we captured the following data:</p>
                            <p>We noticed that the actual bin numbers corresponding to the peaks for the 660Hz, 1320Hz,
                                and
                                1960Hz test tones were 20, 39, and 57 respectively. Although these bins are not exactly
                                what we expected, they are within a margin of error of a bin, which we found acceptable
                                for
                                an Arduino.</p>
                            <p>In our next step of testing, we applied the 660 Hz signal with background noise. To see
                                if
                                we could discern the signal from the noise, we looked at the FFT and checked if the bin
                                with the 660Hz signal is within a certain intensity of the other signal. Please see the
                                below video for a demonstration:</p>
                            <p>
                                <p class="text-center"><iframe width="560" height="315" src="https://www.youtube.com/embed/u_h56zHHiHA"
                                    frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></p>
                            </p>
                            <h3><a id="Optical_114"></a>Optical</h3>
                            <p>We first set up the photo-transistor circuit as depicted in the lab handout:</p>
                            <p class="text-center"><img style="max-width:100%; max-height: 400px;" src="https://github.com/Blue9/ece3400-team20/blob/gh-pages/img/portfolio/phototransistor.png?raw=true"
                                    alt="Photo-transistor circuit"></p>
                            <p>We connected it to a 5V Power Source and then connected the IR Hat to 9 Volts. To
                                increase
                                the IR sensor’s range of detection, we added a simple inverting op-amp circuit with a
                                gain
                                of about -100 utilizing resistor values of 100,000 and 1,000 Ohms respectively. See the
                                following article for the circuit design we referenced and built: <a href="https://en.wikipedia.org/wiki/Operational_amplifier_applications#Inverting_amplifier">Inverting
                                    Amplifier</a>. This circuit is shown below but we later found that its benefits
                                were
                                minimal for our practical purposes. And so we decided not to use it in our final
                                design.</p>
                            <p class="text-center"><img style="max-width:100%; max-height: 400px;" src="https://github.com/Blue9/ece3400-team20/blob/gh-pages/img/OpAmpCircuitOptical.png?raw=true"
                                    alt="Op-amp circuit"></p>
                            <p>The next thing that we tested was utilizing the FFT library in order to detect a 6080 Hz
                                signal once the Arduino received the signal. We opened the example sketch:
                                “fft_adc_serial”
                                from the Open Music Labs Library and determined whether it was detecting frequencies
                                from
                                the IR Hat. What we noticed is that that the Arduino was printing values between 0 to
                                256
                                onto the Serial Monitor to denote the value of each of the 256 buckets for the Fourier
                                Transform it was computing.</p>
                            <p>Once we made that observation, we changed the sampling rate, according to the
                                calculations
                                done by the Team Alpha report, by setting the ADCSRA register to 0xe4 from the standard
                                fft_adc_serial script. This makes the ADC sample rate to be about 76,800 Hz. This was
                                done
                                since we needed to sample above 36,000 Hz to detect decoys at 18,000 Hz according to
                                the
                                Nyquist condition and this was smallest sample rate above that constraint. We often
                                printed
                                these Fourier transforms to the Serial monitor to see if we were detecting the correct
                                bin/frequency.</p>
                            <p>We then developed a function denoted as <code>isSignalThere</code> to calculate whether
                                the
                                FFT contained a peak at the frequency we desired. This function takes in the target
                                frequency, the sampling rate, and the FFT array as parameters. The width of each bin is
                                the
                                sampling frequency divided by 256, and the function calculates which bin the target
                                frequency is and then checks whether this bin or its adjacent bins contain a value
                                above a
                                threshold, which was determined through empirical tests to be 60.</p>
                            <pre><code>
<!--                         -->boolean isSignalThere(uint8_t fft[], int targetFrequency, long samplingRate) {
<!--                         -->    long bucketLength = samplingRate / 256;
<!--                         -->    uint8_t maxValue = 0;
<!--                         -->
<!--                         -->    uint8_t threshold = 60; 
<!--                         -->    int targetBucket = targetFrequency / bucketLength;
<!--                         -->    byte width = 2;
<!--                         -->    for (byte i = 0; i &lt; width; i++) {
<!--                         -->    if (fft[targetBucket + i] &gt;= threshold || fft[targetBucket - i] &gt;= threshold) {
<!--                         -->        return true;
<!--                         -->    }
<!--                         -->    }
<!--                         -->    return false;
<!--                         -->}
                            </code></pre>
                            <p>Utilizing this function along with the <code>fft_adc_serial</code> example sketch by
                                calling
                                it with the parameters targetFrequency = 6080 and sampling rate = 76,800, we were able
                                to
                                print to the serial monitor whether we were detecting a signal at 6080 Hz. The
                                following
                                video shows how we were able to detect cycling the IR Hat on and off using the Serial
                                plotter from a foot away:</p>
                            <p>
                                <p class="text-center"><iframe width="560" height="315" src="https://www.youtube.com/embed/_tRcdDt8q-A"
                                    frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></p>
                            </p>
                            <p>We then tested the detection with an IR Decoy that was initially set to 12,000 Hz, later
                                set
                                to 18,000 Hz, and natural/flourescent light to make sure only the 6080 Hz tone was
                                detected.</p>
                            <p>The following video shows how we were able to detect the 6080 Hz signal, but not the
                                decoy
                                when it was placed in the way:</p>
                            <p>
                                <p class="text-center"><iframe width="560" height="315" src="https://www.youtube.com/embed/_UgpGABSOR8?start=17"
                                    frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></p>
                            </p>
                            <p>Finally, the IR Hat and decoy were displaced slightly so the IR sensor sensed both
                                signals,
                                but only detected the 6080Hz tone. The FFT of the two signals is below:</p>
                            <p class="text-center"><img style="max-width:100%; max-height: 400px;" src="https://github.com/Blue9/ece3400-team20/blob/gh-pages/img/portfolio/fftpicopticalteam%20(1).jpg?raw=true"
                                    alt="FFT"></p>
                            <h3><a id="Combined_System_162"></a>Combined System</h3>
                            <p>Once each system had been implemented and tested they were combined into a single
                                Arduino
                                sketch, with the optical signal reading from A0, and the acoustic signal being read
                                from
                                A5. The loop for individual system was put into a function, and were then called from a
                                top
                                level loop, as seen below:</p>
                            <pre><code>
<!--                         -->void loop() {
<!--                         -->  while(1) { // reduces jitter
<!--                         -->    bool audio    = audioFFT();
<!--                         -->    bool optical  = opticalFFT();
<!--                         -->    
<!--                         -->    Serial.print(&quot;Optical: &quot;);
<!--                         -->    Serial.print(optical);
<!--                         -->    Serial.print(&quot;\tAudio: &quot;);
<!--                         -->    Serial.println(audio);
<!--                         -->
<!--                         -->  }
<!--                         -->}
                            </code></pre>
                            <p>Simply pasting the two sets of code into functions do not work properly, as they use
                                different sample rates and ADC settings. To combat this issue initialization of the ADC
                                was
                                moved form the <code>setup()</code> function to each of the individual FFT functions.
                                After
                                this change was made the entire system worked properly as shown in the video below:</p>
                            <p>
                                <p class="text-center"><iframe width="560" height="315" src="https://www.youtube.com/embed/Ro4x0O4s6_g"
                                    frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></p>
                            </p>
                        </div>
                        <a class="btn btn-primary btn-lg rounded-pill portfolio-modal-dismiss" href="#">
                            <i class="fa fa-close"></i> Close Project</a>
                    </div>
                </div>
            </div>
        </div>
    </div>
    <!-- Portfolio Modal 3 -->
    <div class="portfolio-modal mfp-hide" id="portfolio-modal-3">
        <div class="portfolio-modal-dialog bg-white">
            <a class="close-button d-none d-md-block portfolio-modal-dismiss" href="#">
                <i class="fa fa-3x fa-times"></i>
            </a>
            <div class="container text-center">
                <div class="row">
                    <div class="col-lg-10 mx-auto">
                        <h2 class="text-secondary text-center text-uppercase mb-0">Lab 3</h2>
                        <hr class="star-dark mb-5">
                        <div class="text-left">

                        <p><em>Objective</em>: The goal of this lab was to integrate all of the components of previous
                            labs and milestones into a single working system, as well as develop new functionality to
                            transmit maze data between the robot and a base station that will interface with a GUI,
                            displaying the maze.</p>
                        <h3><a id="Setup_4"></a>Setup</h3>
                        <p>The team was split up into two groups, one to work on development of the radio communication
                            and corresponding protocols, and the other to integrate the previous components together.</p>
                        <h3><a id="Radio_8"></a>Radio</h3>
                        <h5><a id="Getting_Started_with_the_Radios_10"></a>Getting Started with the Radios</h5>
                        <p>We started out by connecting the radios to the special green board and then to the 3.3 Volt
                            Power Supply.<br>
                            We then downloaded the RF24 Library off of &quot;<a href="https://github.com/maniacbug/RF24">https://github.com/maniacbug/RF24</a>&quot;.
                            Once we downloaded the ZIP file, we included the library by navigating to the Arduino
                            application and then to “Sketch” -&gt; “Include Library” -&gt; “Add .Zip File”.</p>
                        <p>We then downloaded the “Getting Started” sketch off of the ECE 3400 Github at<br>
                            &quot;<a href="https://github.com/CEI-lab/ece3400-2018/blob/master/docs/Solutions/lab4/GettingStarted/GettingStarted.ino">https://github.com/CEI-lab/ece3400-2018/blob/master/docs/Solutions/lab4/GettingStarted/GettingStarted.ino</a>&quot;.
                            What we immediately noticed is that it would not compile as a result of the “printf”
                            statements. Thus, we replaced each of the “printf” statements with “Serial.println” and we
                            were able to get the code to compile and upload.</p>
                        <p>At first, we had difficulties with transmitting and receiving utilizing the “Getting
                            Started” sketch. Utilizing a Voltmeter,<br>
                            we recognized that there was a short within the radio that was causing issues and so we had
                            to borrow another groups’ radio in order to continue.</p>
                        <p>We then set our pipe values by utilizing the given formula: 2(3D + N) + X where D = 3 and N
                            = 20 represented our lab date and group number. This yielded 58 and 59 which in hexadecimal
                            are 3A And 3B. And so we changed the pipe values to 0x000000003ALL<br>
                            and 0x000000003BLL respectively.</p>
                        <p>This allowed us to get the “Getting Started” sketch to work with an accuracy of
                            approximately 8 feet from each other.</p>
                        <h4><a id="Transmitting_Robot_Information_26"></a>Transmitting Robot Information</h4>
                        <p>At this point, we needed to develop a method of transmitting data about our surroundings and
                            a way of encoding our surroundings. We decided on the following 2 byte system:</p>
                        <table class="table table-striped table-bordered">
                            <thead>
                                <tr>
                                    <th style="text-align:center">Bit</th>
                                    <th>Description</th>
                                </tr>
                            </thead>
                            <tbody>
                                <tr>
                                    <td style="text-align:center">15-13</td>
                                    <td>X Coordinate</td>
                                </tr>
                                <tr>
                                    <td style="text-align:center">12-10</td>
                                    <td>Y Coordinate</td>
                                </tr>
                                <tr>
                                    <td style="text-align:center">9</td>
                                    <td>iamhere (robot is here)</td>
                                </tr>
                                <tr>
                                    <td style="text-align:center">8</td>
                                    <td>west (whether there is a west wall)</td>
                                </tr>
                                <tr>
                                    <td style="text-align:center">7</td>
                                    <td>north (whether there is a north wall)</td>
                                </tr>
                                <tr>
                                    <td style="text-align:center">6</td>
                                    <td>east (whether there is a east wall)</td>
                                </tr>
                                <tr>
                                    <td style="text-align:center">5</td>
                                    <td>south (whether there is a south wall)</td>
                                </tr>
                                <tr>
                                    <td style="text-align:center">4</td>
                                    <td>robot (whether there is a robot)</td>
                                </tr>
                                <tr>
                                    <td style="text-align:center">3-1</td>
                                    <td>treasure (which treasure we detected)</td>
                                </tr>
                                <tr>
                                    <td style="text-align:center">0</td>
                                    <td>Unused</td>
                                </tr>
                            </tbody>
                        </table>
                        <p>Thus, we created the global variables x_cord, y_cord, iamhere, west, north, east, south, and
                            robot and depending on whether these values were true or not, we encoded an appropriate 2
                            byte value (short) to send. We created the following method “get_msg” to convert these
                            global variables to the encoded message. We also created an enum for treasures that
                            contained “R_C, R_T, R_S, B_C, B_T, B_S, and NaN” that tsure would take on the values of.
                            The method works as follows:</p>
                        <pre><code>unsigned short get_msg(){
<!--                         -->    unsigned short t;
<!--                         -->    switch (tsure){
<!--                         -->        case R_C : t = 0b110; break;
<!--                         -->        case R_T : t = 0b101; break;
<!--                         -->        case R_S : t = 0b100; break;
<!--                         -->        case B_C : t = 0b011; break;
<!--                         -->        case B_T : t = 0b010; break;
<!--                         -->        case B_S : t = 0b001; break;
<!--                         -->        default : t = 0b000; break;
<!--                         -->    }
<!--                         -->        
<!--                         -->    unsigned short msg = (x_coord &lt;&lt; 13) + (y_coord &lt;&lt; 10) +
<!--                         -->                (iamhere  &lt;&lt; 9) +
<!--                         -->                (west     &lt;&lt; 8) +
<!--                         -->                (north    &lt;&lt; 7) +
<!--                         -->                (east     &lt;&lt; 6) +
<!--                         -->                (south    &lt;&lt; 5) +
<!--                         -->                (robot    &lt;&lt; 4) +
<!--                         -->                (t        &lt;&lt; 1);
<!--                         -->
<!--                         -->    return t;
<!--                         -->}</code></pre>
                        <p>This allowed us to encode messages to write to our radios that would be able to transmit to
                            each other radio.</p>
                        <h4><a id="Receiving_Information_73"></a>Receiving Information:</h4>
                        <p>Once a radio received a packet in the data scheme above, we decoded the information
                            utilizing bit shifting.<br>
                            For example, in order to obtain the x and y coordinates, we used the following masks and
                            bit shifting to decode the values:</p>
                        <pre><code>#define x_mask 0xe000
#define x_shift 13
#define y_mask 0x1c00
#define y_shift 10
#define get_x(a) (a&amp;x_mask) &gt;&gt; x_shift
#define get_y(a) (a&amp;y_mask) &gt;&gt; y_shift
</code></pre>
                        <p>This allowed us to communicate information from one radio to another and decode it
                            appropriately. The following images demonstrates the output when we sent encoded
                            information from one radio to another:</p>

                        <img src="https://github.com/Blue9/ece3400-team20/blob/gh-pages/img/IMG_5380.jpg?raw=true"
                            style="width:500px">
                        <br>
                        <img src="https://github.com/Blue9/ece3400-team20/blob/gh-pages/img/IMG_5381.jpg?raw=true"
                            style="width:500px; margin: 10px auto;">
                        <br>
                        <h4><a id="Updating_GUI_from_the_base_station_92"></a>Updating GUI from the base station</h4>
                        <p>To update the GUI from the base station we first figured out which port the Arduino was
                            connected to as well as the name of the tty shell the serial monitor writes to. We then
                            hardcoded values representing a virtual maze and ran the program. We monitored the GUI to
                            see updates. As expected, the GUI updated to reflect the explored maze.</p>
                        <h4><a id="Interfacing_with_the_GUI_and_Wireless_Communication_97"></a>Interfacing with the GUI
                            and Wireless Communication</h4>
                        <p>Once we were able to send maze information from one Arduino to another, we needed to
                            interface with the GUI. This<br>
                            involved taking the parsed information and turning the information into a formatted string
                            that the GUI would accept to the serial monitor. An example of a string is as follows:
                            “2,3,west=false,north=true,south=false,robot=true.” We were able to generate this string by
                            utilizing tertiary conditionals i.e. if the west bit is 1 then add “true” to the output
                            string and if it is 0 then add “false”.</p>
                        <p>And so once we generated this string from the parsed information, we were able to generate
                            images on the GUI. In order to create a virtual simulation, we created an array of virtual
                            information that the transmitter would iterate through, the receiver would parse and update
                            the GUI with. We manually created this array. The simulation array we used was the
                            following based on the transmission scheme decoded above:</p>
                        <pre><code>{0b0000001110100000, 0b0000011010100000, 0b0000101011000110, 0b0010101001100000, 0b0010011010101010, 0b0010001110000000, 0b0100001100100000, 0b0100011010100000, 0b0100101010100000};
</code></pre>
                        <p>A wireless virtual transmission with an interface with the GUI is presented below. There is
                            slight lagging mostly due to the browser’s refreshing rate.</p>
                        <p>
                            <p class="text-center"><iframe width="560" height="315" src="https://www.youtube.com/embed/IlqivRq3zKw"
                                frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></p>
                        </p>
                        <h4><a id="Efficient_data_scheme_to_store_all_maze_information_on_an_Arduino_112"></a>Efficient
                            data scheme to store all maze information on an Arduino</h4>
                        <p>We used an 8x8 array to store maze information. Each index corresponds to an intersection.
                            To keep the data at a minimum, we stored a two byte short in each index of the array. Each
                            packet sent wirelessly is decoded and stored in the array.Below is an outline of each bit:</p>
                        <h3><a id="Robot_117"></a>Robot</h3>
                        <h4><a id="Audio_FFT_119"></a>Audio FFT</h4>
                        <p>To begin we incorporated the analog audio circuit from Lab 2 onto the primary breadboard,
                            and connecting it to one of the analog input pins. For more information on how the audio
                            circuit is implemented please see ** Lab 2 **.</p>
                        <p>An additional state in our FSM was created as the initial state which waits, continuously
                            checking the <code>audioFFT()</code>, also described in Lab 2, looking for the 660Hz start
                            signal. Upon detecting the signal, the FSM transitions to moving forward and begins
                            traversing the maze via the right hand rule.</p>
                        <p>Once this state was implemented we noticed that the servos jittered in the initial state
                            when <code>audioFFT()</code> was run. After some testing we discovered this was because the
                            <code>audioFFT()</code> function disables the timers and interrupts used by the <code>Servo</code>
                            class.<br>
                            The following changes resolved this issue:</p>
                        <pre><code>// cli(); // &lt;- commented out
...
// sei(); // &lt;- commented out
</code></pre>
                        <p>After this change was made the latency of the <code>audioFFT()</code> was drastically
                            reduced so we had to increase our threshold for the number of sequential FFTs required to
                            detect the start tone from 5 to 15. This ensure that common speech or music did not set off
                            the robot.<br>
                            Below is a video of the robot starting on a tone:</p>
                        <p>Robot starting on a 660Hz tone</p>
                        <p>
                            <p class="text-center"><iframe width="560" height="315" src="https://www.youtube.com/embed/J3oYayUAUiY"
                                frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></p>
                        </p>
                        <p>In case the start tone detection fails, we also incorporated a manual override switch that
                            can start the robot during the competition.<br>
                            Below is a video of the robot starting via manual override:</p>
                        <p>Robot starting from manual override switch:</p>
                        <p>
                            <p class="text-center"><iframe width="560" height="315" src="https://www.youtube.com/embed/3r8BT0jyPSM"
                                frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></p>
                        </p>
                        <h4><a id="Left_Wall_Sensor_147"></a>Left Wall Sensor</h4>
                        <p>Previously, we used two sensors to detect walls: one for detecting walls ahead and one for
                            detecting walls to the right. We left out a left-wall sensor because we did not have enough
                            analog pins, and left wall detection only mattered in the case of a dead-end, in which case
                            our robot would begin to turn, detect the wall on the left (near the end of the turn), and
                            “undo” the turn by reversing out. In this lab, since mapping the walls is needed, we
                            decided to add a left-wall sensor by muxing our wall sensor inputs. After building the
                            necessary circuitry for the mux, we built a few methods for retrieving the various wall
                            sensor values. A code snippet is included below.</p>
                        <pre><code>int sensor_at_wall(int mux_control, int threshold) {
  digitalWrite(MUX_PIN_0, mux_control &amp; 0b01);
  digitalWrite(MUX_PIN_1, (mux_control &amp; 0b10) &gt;&gt; 1);
  return analogRead(WALL_MUX_PIN_IN) &gt; threshold;
}
</code></pre>
                        <p><code>MUX_PIN_0</code> is the LSB of the mux control signal and <code>MUX_PIN_1</code> is
                            the MSB. Since we had already abstracted away the hardware implementation of the wall
                            sensors in methods, we had to make minimal changes to our code to account for this change.</p>
                        <p><em>Note:</em> The mux used was the <a href="http://www.ti.com/lit/ds/symlink/cd74hc4051.pdf">TI
                                CD74HCT4051</a> with the S2 select grounded, making it effectively a 4:1 mux.</p>
                        <h4><a id="Maze_Mapping_162"></a>Maze Mapping</h4>
                        <p>The robot will eventually need to plan its path around the maze using a more substantial
                            algorithm than right-hand wall following. To do this, it will need information about the
                            maze as a whole stored locally on the Arduino. Since there are 64 total intersections, this
                            information must be stored as efficiently as possible.<br>
                            To this end, we designed a new encoding scheme to record information about the walls and
                            treasures located at each intersection. All of this information can be stored in two bytes
                            per intersection as follows:</p>
                        <table class="table table-striped table-bordered">
                            <thead>
                                <tr>
                                    <th style="text-align:center">Bit</th>
                                    <th>Description</th>
                                </tr>
                            </thead>
                            <tbody>
                                <tr>
                                    <td style="text-align:center">15-11</td>
                                    <td>Unused</td>
                                </tr>
                                <tr>
                                    <td style="text-align:center">10-9</td>
                                    <td>Treasure Shape</td>
                                </tr>
                                <tr>
                                    <td style="text-align:center">8</td>
                                    <td>West Valid</td>
                                </tr>
                                <tr>
                                    <td style="text-align:center">7</td>
                                    <td>West</td>
                                </tr>
                                <tr>
                                    <td style="text-align:center">6</td>
                                    <td>North Valid</td>
                                </tr>
                                <tr>
                                    <td style="text-align:center">5</td>
                                    <td>North</td>
                                </tr>
                                <tr>
                                    <td style="text-align:center">4</td>
                                    <td>East Valid</td>
                                </tr>
                                <tr>
                                    <td style="text-align:center">3</td>
                                    <td>East</td>
                                </tr>
                                <tr>
                                    <td style="text-align:center">2</td>
                                    <td>South Valid</td>
                                </tr>
                                <tr>
                                    <td style="text-align:center">1</td>
                                    <td>South</td>
                                </tr>
                                <tr>
                                    <td style="text-align:center">0</td>
                                    <td>Unused</td>
                                </tr>
                            </tbody>
                        </table>
                        <p>The valid bits allow us to record information about walls at intersections that we have not
                            explored yet. For example, assuming 0,0 is the northwest intersection, if we find a
                            northern wall at intersection 2,3, we know that there is a southern wall at 2,2. In this
                            case, we would set the south bit and the south valid bit of the 2,2 intersection, but the
                            other walls would remain invalid. That way, the robot can reason with the maximum amount of
                            data available. We do not believe that the redundancy is a significant problem, as this
                            only adds a few extra bytes of information total. The data structure as a whole only
                            occupies 128 bytes, so it should not significantly affect our memory usage. Updating this
                            data structure only involves a single call to the array_update function per intersection,
                            so it shouldn’t be a significant drain on processing power either.</p>
                        <h4><a id="Testing_With_Other_Robots_183"></a>Testing With Other Robots</h4>
                        <p>From milestone 2, we were able to test our IR hat and optical FFT implementation. In this
                            lab, we mounted the IR hat at a 5.5 inch height and connected a 9 V source to it. To make sure our
                            robot could avoid other robots, we placed another team's robot in the maze and made sure our robot
                            avoided it. Additionally, we made sure to disable front wall detection while conducting this test
                            to make sure the robot was actually detecting the other robot and not just a wall. A video of this
                            is shown below.
                            
                            <p>
                                <p class="text-center"><iframe width="560" height="315" src="https://www.youtube.com/embed/9wIbhgJp84U"
                                    frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></p>
                            </p>
                            
                            <p>Regarding decoys, we do not actively seek out decoys and ignore them. This is because our
                            optical FFT functions are accurate enough to only return true when an actual robot is
                            detected.
                            </p>
                            
                        <h3><a id="Complete_Integration_186"></a>Complete Integration</h3>
                        <p>After all of the aforementioned changes had be integrated into the design we did a final
                            test of the robot starting on a tone, navigating a maze, storing the maze locally, and
                            transmitting the maze information wirelessly to the base station which interfaced with the
                            GUI. Below is a video of this in action:</p>
                        <p>
                            <p class="text-center"><iframe width="560" height="315" src="https://www.youtube.com/embed/a42s1vxHHX8"
                                frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></p>
                        </p>

                        </div>
                        <a class="btn btn-primary btn-lg rounded-pill portfolio-modal-dismiss" href="#">
                            <i class="fa fa-close"></i> Close Project</a>
                    </div>
                </div>
            </div>
        </div>
    </div>
    <!-- Portfolio Modal 4 -->
    <div class="portfolio-modal mfp-hide" id="portfolio-modal-4">
        <div class="portfolio-modal-dialog bg-white">
            <a class="close-button d-none d-md-block portfolio-modal-dismiss" href="#">
                <i class="fa fa-3x fa-times"></i>
            </a>
            <div class="container text-center">
                <div class="row">
                    <div class="col-lg-10 mx-auto">
                        <h2 class="text-secondary text-uppercase mb-0">Lab 4</h2>
                        <hr class="star-dark mb-5">
                        <div class="text-left">
                                <h3><a id="FPGA__Shape_Detection_1"></a>FPGA &amp; Shape Detection</h3>
                                <p><em>Objective:</em> The goal of this lab was to develop software and hardware to interface with the OV7670 Camera and the DE0-NANO FPGA. To accomplish this the team split up into two groups, one to focus on the communication between the camera and FPGA with the Arduino, and the other to focus on streaming the video between the camera and FPGA.</p>
                                <h4><a id="Prelab_4"></a>Prelab</h4>
                                <p>Before splitting up into teams, we worked together to finish prelab, finding the key registers on the OV7670 to configure its settings. To know what values to write, we first looked at the user manual for the <a href="http://www.ti.com/lit/ug/tidu737/tidu737.pdf">DE0-NANO FPGA</a>. The buffer needs to hold an entire frame of 8b pixels so at 594kb there can be a total of ~74,250 total pixels per frame. This led us to picking the format of the camera to be QCIF leading to a resolution of 176x144, equaling 25,344 pixels per frame.</p>
                                <p>Once the resolution was known we could continue to find all of registers to initialize the camera. The following table shows the required registers and their corresponding values:</p>
                                <table class="table table-striped table-bordered">
                                <thead>
                                <tr>
                                <th>Description</th>
                                <th style="text-align:center">Register</th>
                                <th style="text-align:center">Value</th>
                                </tr>
                                </thead>
                                <tbody>
                                <tr>
                                <td>Reset Regs</td>
                                <td style="text-align:center">0x12</td>
                                <td style="text-align:center">0x80</td>
                                </tr>
                                <tr>
                                <td>Enable Scaling</td>
                                <td style="text-align:center">0x0C</td>
                                <td style="text-align:center">0x04</td>
                                </tr>
                                <tr>
                                <td>Use External CLK</td>
                                <td style="text-align:center">0x11</td>
                                <td style="text-align:center">0x40</td>
                                </tr>
                                <tr>
                                <td>Set Resolution</td>
                                <td style="text-align:center">0x12</td>
                                <td style="text-align:center">0x14</td>
                                </tr>
                                <tr>
                                <td>Enable Color Bar Test</td>
                                <td style="text-align:center">0x12</td>
                                <td style="text-align:center">0x02</td>
                                </tr>
                                <tr>
                                <td>Vertical Flip</td>
                                <td style="text-align:center">0x1E</td>
                                <td style="text-align:center">0x10</td>
                                </tr>
                                <tr>
                                <td>Mirror Flip</td>
                                <td style="text-align:center">0x1E</td>
                                <td style="text-align:center">0x20</td>
                                </tr>
                                </tbody>
                                </table>
                                <p><em>Note: Some use the same register so the required value cannot simply overwrite the old, but more consideration is required. Implementation is described in the <strong>Team Arduino</strong> section.</em></p>
                                <p>The values for the above table were found in the <a href="https://www.voti.nl/docs/OV7670.pdf">OV7670 Datasheet</a>.</p>
                                <h4><a id="PLL_23"></a>PLL</h4>
                                <p>Before spliting up in teams we generate a PLL module to create multiple clocked signals for use both internal and external to the FPGA. The PLL module was generate using Quartus’s interal IP generation. We followed the <a href="https://github.com/CEI-lab/ece3400-2018/blob/master/docs/lab4.md#pll">Lab 4 handout</a> PLL section initially, however, we added a fourth clock signal at 8MHz for use with the serial communication between the FPGA and Arduino. The use of this will be described more in the <strong>Team Arduino</strong> section.</p>
                                <h3><a id="Team_Arduino_26"></a>Team Arduino</h3>
                                <p>The goal of team Adruino was to wire the camera, initialize all of its registers, and interface with the FPGA to read treasure data. To begin we downloaded the <a href="https://cei-lab.github.io/ece3400-2018/Lab4_Arduino_Template.zip">OV7670 setup sketch</a> as a baseline for implementation.</p>
                                <h4><a id="Internal_PullUp_Disable_28"></a>Internal Pull-Up Disable</h4>
                                <p>The first step in initializing the camera is wiring the Arduino to the OV7670, however, the Arduino operates on 5V and the camera operates on 3.3V, so the internal pull-ups to the Arduino’s <code>SDA</code> and <code>SCL</code> pins needed to be disable to avoid burning out the camera.</p>
                                <p><em>Note: This must be done <strong>EVERY</strong> time <strong>BEFORE</strong> code is uploaded.</em></p>
                                <p>To disable the settings on a Windows PC:</p>
                                <ol>
                                <li>Go to <code>C:\Program Files (x86)\Arduino\hardware\arduino\avr\libraries\Wire\src\utility</code></li>
                                <li>Right click on <em>twi.c</em> and select <em>Properties&gt;Securities&gt;Edit</em></li>
                                </ol>
                                <ul>
                                <li>Click <em>Users</em></li>
                                <li>Click <em>Full Control</em></li>
                                <li>Click <em>OK</em> and <em>Close</em></li>
                                </ul>
                                <ol start="3">
                                <li>Open <em>twi.c</em></li>
                                <li>Comment out the following lines:</li>
                                </ol>
<!--                         --><pre><code class="language-C"><span class="hljs-comment">//activate internal pullups for twi</span>
<!--                         -->digitalWrite(SDA,<span class="hljs-number">1</span>);
<!--                         -->digitalWrite(SCL,<span class="hljs-number">1</span>);
                                </code></pre>
                                <ol start="5">
                                <li>Save and close</li>
                                </ol>
                                <p>We found that even after changing the file permissions, sometimes users were not able to modify <em>twi.c</em>. To work around this we copied the contents of the <em>twi.c</em> to a new file in a different directory and named the new file <em>twi.c</em>. Then, the new <em>twi.c</em> was copied into <code>C:\Program Files (x86)\Arduino\hardware\arduino\avr\libraries\Wire\src\utility</code>, replacing the old <em>twi.c</em> with the new modified version.</p>
                                <h4><a id="Physical_Setup_52"></a>Physical Setup</h4>
                                <p>After internal pull-ups were disabled the OV7670 and Arduino were wired based on the labs suggestion:</p>
                                <p class="text-center"><img src="https://cei-lab.github.io/ece3400-2018/images/Lab3CameraWiringDiagram.png" alt="OV7670_Wiring"></p>
                                <p>The wire names in the above picture do <em>NOT</em> directly map to the pinout on the OV7670. The conversions are as follows:</p>
                                <table class="table table-striped table-bordered">
                                <thead>
                                <tr>
                                <th style="text-align:center">OV7670 Pin Name</th>
                                <th style="text-align:center">Signal Name</th>
                                </tr>
                                </thead>
                                <tbody>
                                <tr>
                                <td style="text-align:center">SIOC</td>
                                <td style="text-align:center">SCL</td>
                                </tr>
                                <tr>
                                <td style="text-align:center">SIOD</td>
                                <td style="text-align:center">SCA</td>
                                </tr>
                                <tr>
                                <td style="text-align:center">MCLK</td>
                                <td style="text-align:center">XCLK</td>
                                </tr>
                                </tbody>
                                </table>
                                <p>To generate the 24MHz signal from the FPGA, the output of the PLL module described above was mapped to a GPIO output pin:</p>
                                <pre><code class="language-Verilog"><span class="hljs-keyword">assign</span> GIPO_0_D[<span class="hljs-number">0</span>] <span class="hljs-keyword">=</span> CLK_24;
                                </code></pre>
                                <h4><a id="Communicating_with_the_Camera_69"></a>Communicating with the Camera</h4>
                                <p>To setup an I2C connection the first step is finding the address of the slave, in this case the OV7670. In the datasheet it states for writing the addres is <code>0x42</code> and <code>0x43</code> for reading, leading to a base address of <code>0x21</code> and an appended <code>0</code> for write and <code>1</code> for read.</p>
                                <p>To initialize the camera, the results from the prelab were used, writing the required values to the identified registers. To write the data the provided function <code>OV7670_write_register(reg_address, data)</code> was used, however, just setting the bits we need would overwrite the values of all other bits in the registers with <code>0</code>. Instead, we OR’d the current value of the register written to with the desired value preserving the other bits. For example:</p>
                                <pre><code class="language-C">OV7670_write_register(<span class="hljs-number">0x01</span>, (read_register_value(<span class="hljs-number">0x02</span>) | <span class="hljs-number">0x03</span>));
                                </code></pre>
                                <p>This was particularly relevant for certain registers were we needed to set multiple bits for initialization.</p>
                                <p>After all registers were written, the bits that were intended to be set were checked to ensure proper setup. This was done by reading the value of a register, ANDing it wih the bit in question, and comparing the value to the expected:</p>
                                <pre><code class="language-C">correct = ((read_register_value(<span class="hljs-number">0x12</span>) &amp; <span class="hljs-number">0x80</span>) == <span class="hljs-number">0x80</span>);
                                </code></pre>
                                <p>The setup is only deemed correct if all registers have the correct value.</p>
                                <h4><a id="Communicating_with_the_FPGA_84"></a>Communicating with the FPGA</h4>
                                <p>To begin a protocol needed to be designed to communicate with between the FPGA and Arduino. Due to the contrainst of digital pins on the Ardunio, we attempted to create a serial communication protocol.</p>
                                <h5><a id="FPGA__Serial_87"></a>FPGA - Serial</h5>
                                <p>To ensure no bits are ever missed the 8MHz signal generated in the PLL module was output to a digital pin for the clock signal for serial communication:</p>
                                <pre><code class="language-Verilog"><span class="hljs-keyword">assign</span> GPIO_0_D[<span class="hljs-number">1</span>] <span class="hljs-keyword">=</span> CLK_8;
                                </code></pre>
                                <p>This is half the frequency of the Arduino so there signal will always be detected unless there is outside interference.</p>
                                <p>The protocol transmits a single byte with bits (2:0) representing the treasures as follows:</p>
                                <table class="table table-striped table-bordered">
                                <thead>
                                <tr>
                                <th style="text-align:left">Treasure</th>
                                <th style="text-align:center">Value</th>
                                </tr>
                                </thead>
                                <tbody>
                                <tr>
                                <td style="text-align:left">None</td>
                                <td style="text-align:center">0x0</td>
                                </tr>
                                <tr>
                                <td style="text-align:left">Red Circle</td>
                                <td style="text-align:center">0x1</td>
                                </tr>
                                <tr>
                                <td style="text-align:left">Red Triangle</td>
                                <td style="text-align:center">0x2</td>
                                </tr>
                                <tr>
                                <td style="text-align:left">Red Square</td>
                                <td style="text-align:center">0x3</td>
                                </tr>
                                <tr>
                                <td style="text-align:left">Blue Circle</td>
                                <td style="text-align:center">0x4</td>
                                </tr>
                                <tr>
                                <td style="text-align:left">Blue Triangle</td>
                                <td style="text-align:center">0x5</td>
                                </tr>
                                <tr>
                                <td style="text-align:left">Blue Square</td>
                                <td style="text-align:center">0x6</td>
                                </tr>
                                </tbody>
                                </table>
                                <p>Only three bits are needed to represent the treasure leaving 5 bits for a preamble for the Arduino to accurately find the value. After much consideration the preamble <code>0b00111</code> was chosen as it is always detectable and will never be acciedentally transmitted because of the treasure value.</p>
                                <p>The preamble is concatenated with the treasure data and transmited one bit at a time over a digital GPIO pin:</p>
                                <pre><code class="language-Verilog"><span class="hljs-keyword">assign</span> GPIO_0_D[<span class="hljs-number">3</span>] <span class="hljs-keyword">=</span> cnt[<span class="hljs-number">2</span>] ? 
                                                      (cnt[<span class="hljs-number">1</span>] ? 
                                                        (cnt[<span class="hljs-number">0</span>] ? arduino_output[<span class="hljs-number">7</span>] <span class="hljs-keyword">:</span> arduino_output[<span class="hljs-number">6</span>])  <span class="hljs-keyword">:</span> 
                                                        (cnt[<span class="hljs-number">0</span>] ? arduino_output[<span class="hljs-number">5</span>] <span class="hljs-keyword">:</span> arduino_output[<span class="hljs-number">4</span>])) <span class="hljs-keyword">:</span> 
                                                      (cnt[<span class="hljs-number">1</span>] ? 
                                                        (cnt[<span class="hljs-number">0</span>] ? arduino_output[<span class="hljs-number">3</span>] <span class="hljs-keyword">:</span> arduino_output[<span class="hljs-number">2</span>])  <span class="hljs-keyword">:</span> 
                                                        (cnt[<span class="hljs-number">0</span>] ? arduino_output[<span class="hljs-number">1</span>] <span class="hljs-keyword">:</span> arduino_output[<span class="hljs-number">0</span>]));
                                </code></pre>
                                <p>In the above snippet <code>cnt</code> is a counter which counts from <code>0 -&gt; 7</code> with wrap around, clocked on the 8MHz signal.</p>
                                <h5><a id="Arduino__Serial_120"></a>Arduino - Serial</h5>
                                <p>On the Arduino we utilized the builtin <code>shiftIn()</code> function which is documented <a href="https://www.arduino.cc/reference/en/language/functions/advanced-io/shiftin/">here</a>. In essence, it read a value from a digital pin based on a rising clock edge read from a second digital pin. The data pin for <code>shiftIn()</code> was connected to the GPIO output for data on the FPGA, and the clock pin was connected to the GPIO tied to the 8MHz signal.</p>
                                <p>To avoid using another digital pin on the Arduino we opted to avoid a <em>start</em> signal which signifies the start of a byte, and instead opted to detect the preamble and align in software. To do this, two bytes of data are needed from <code>shiftIn()</code> and the preamble is found. Once the location of the preamble is detected the treasure is found by the looking at the three bits next lowest, with wrap around.</p>
                                <p>For example:</p>
                                <p><strong>BIT</strong></p>
                                <table class="table table-striped table-bordered">
                                <thead>
                                <tr>
                                <th style="text-align:center">15</th>
                                <th style="text-align:center">14</th>
                                <th style="text-align:center">13</th>
                                <th style="text-align:center">12</th>
                                <th style="text-align:center">11</th>
                                <th style="text-align:center">10</th>
                                <th style="text-align:center">09</th>
                                <th style="text-align:center">08</th>
                                <th style="text-align:center">07</th>
                                <th style="text-align:center">06</th>
                                <th style="text-align:center">05</th>
                                <th style="text-align:center">04</th>
                                <th style="text-align:center">03</th>
                                <th style="text-align:center">02</th>
                                <th style="text-align:center">01</th>
                                <th style="text-align:center">00</th>
                                </tr>
                                </thead>
                                <tbody>
                                <tr>
                                <td style="text-align:center">X</td>
                                <td style="text-align:center">X</td>
                                <td style="text-align:center">X</td>
                                <td style="text-align:center"><em>0</em></td>
                                <td style="text-align:center"><em>0</em></td>
                                <td style="text-align:center"><em>1</em></td>
                                <td style="text-align:center"><em>1</em></td>
                                <td style="text-align:center"><em>1</em></td>
                                <td style="text-align:center"><strong>0</strong></td>
                                <td style="text-align:center"><strong>0</strong></td>
                                <td style="text-align:center"><strong>0</strong></td>
                                <td style="text-align:center">X</td>
                                <td style="text-align:center">X</td>
                                <td style="text-align:center">X</td>
                                <td style="text-align:center">X</td>
                                <td style="text-align:center">X</td>
                                </tr>
                                </tbody>
                                </table>
                                <p>The Arduino’s <code>shiftIn()</code> and the FPGA’s <code>cnt</code> are not alligned, so the first bit read by the Arduino is not the start of a byte, however, when reading the two bytes the preamble is detected in bits [12:8] so the treasure info must be stored in [7:5].</p>
                                <h4><a id="Testing__Serial_136"></a>Testing - Serial</h4>
                                <p>To ensure the serial communication was working properly we first looked at the output from the FPGA on an oscilliscope. In the followign image, the blue signal is clock and the yellow is the signal.</p>
                                <p class="text-center"><img src="https://raw.githubusercontent.com/Blue9/ece3400-team20/gh-pages/img/portfolio/FPGA_SERIAL_OUTPUT.jpg" alt="FPGA_SERIAL"></p>
                                <p>The preamble is can be seen and the data is <code>0b010</code> so the entire byte is <code>0b00111010</code> as seen in the above image.</p>
                                <p>This showed us the FPGA portion of the system was working properly. However, when we attempted to read in the values from the Arduino with <code>shiftIn()</code> allignement and incorrect reads were too significant to overcome.</p>
                                <p>After hours of attempting to recitify these error, changing the clock frequency, using different read functions, and using amplifier 3V to 5V circuits, with no success we opted to attempt a parallel solution instead.</p>
                                <h4><a id="FPGA__Parallel_147"></a>FPGA - Parallel</h4>
                                <p>The parallel implementation was much simpler than the serial, outputting each of the three treasure bits to a different pin.</p>
                                <pre><code class="language-Verilog"><span class="hljs-keyword">assign</span> GPIO_0_D[<span class="hljs-number">1</span>] <span class="hljs-keyword">=</span> treasure[<span class="hljs-number">0</span>];
                                <span class="hljs-keyword">assign</span> GPIO_0_D[<span class="hljs-number">3</span>] <span class="hljs-keyword">=</span> treasure[<span class="hljs-number">1</span>];
                                <span class="hljs-keyword">assign</span> GPIO_0_D[<span class="hljs-number">5</span>] <span class="hljs-keyword">=</span> treasure[<span class="hljs-number">2</span>];
                                </code></pre>
                                <p>The representation for treasures was the same as for serial:</p>
                                <table class="table table-striped table-bordered">
                                <thead>
                                <tr>
                                <th style="text-align:left">Treasure</th>
                                <th style="text-align:center">Value</th>
                                </tr>
                                </thead>
                                <tbody>
                                <tr>
                                <td style="text-align:left">None</td>
                                <td style="text-align:center">0x0</td>
                                </tr>
                                <tr>
                                <td style="text-align:left">Red Circle</td>
                                <td style="text-align:center">0x1</td>
                                </tr>
                                <tr>
                                <td style="text-align:left">Red Triangle</td>
                                <td style="text-align:center">0x2</td>
                                </tr>
                                <tr>
                                <td style="text-align:left">Red Square</td>
                                <td style="text-align:center">0x3</td>
                                </tr>
                                <tr>
                                <td style="text-align:left">Blue Circle</td>
                                <td style="text-align:center">0x4</td>
                                </tr>
                                <tr>
                                <td style="text-align:left">Blue Triangle</td>
                                <td style="text-align:center">0x5</td>
                                </tr>
                                <tr>
                                <td style="text-align:left">Blue Square</td>
                                <td style="text-align:center">0x6</td>
                                </tr>
                                </tbody>
                                </table>
                                <h4><a id="Arduino__Parallel_166"></a>Arduino - Parallel</h4>
                                <p>The Arduino parallel implementation again was much easier than its serial counterpart.</p>
                                <p>The value of the three digital pins the FPGA was connected to were read, representing <code>treasure[i]</code>. From there, the three bits read in were converted to decimal:</p>
                                <pre><code class="language-C"><span class="hljs-number">4</span>*digitalRead(<span class="hljs-number">5</span>) + <span class="hljs-number">2</span>*digitalRead(<span class="hljs-number">6</span>) + digitalRead(<span class="hljs-number">7</span>);
                                </code></pre>
                                <p>Then the table from the <strong>FPGA - Parallel</strong> section was used to translate the decimal to treasures. The final component was requiring the treasure value to stay static to avoid any interference and potential incorrect signals on the line.</p>
                                <pre><code class="language-C"><span class="hljs-keyword">if</span>(trs == prev_trs)
                                  count++;
                                <span class="hljs-keyword">else</span>
                                  count = <span class="hljs-number">0</span>;
                                <span class="hljs-keyword">if</span>(count == trs_threshold)
                                  <span class="hljs-comment">/* display/report treasure value */</span>
                                </code></pre>
                                <h4><a id="Testing__Parallel_183"></a>Testing - Parallel</h4>
                                <p>The following shows the FPGA communicating with Arduino with arbitrary treasure values:</p>
                                <p>
                                    <p class="text-center"><iframe width="560" height="315" src="https://www.youtube.com/embed/t0z3kFzrsgU" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe></p>
                                </p>
                                <h1><a id="Team_FPGA_Setup_188"></a>Team FPGA Setup:</h1>
                                <p>After opening up the Verilog project code from the ECE 3400 website and setting up the clocks as instructed, we declared red, green, and blue as localparameters as 8’b11100000, 8’b00011100, and 8’b00000111 respectively. We also attached the VGA adapter to GPIO_0_5 -&gt; GPIO_0_23 using page 18 of the DEO_NANO user manual. We display the diagram for future reference:</p>
                                <p class="text-center"><img src="https://github.com/Blue9/ece3400-team20/blob/gh-pages/img/portfolio/FPGADiagram.png?raw=true" alt="fpga diagram"></p>
                                <p>We set the pixel that we are writing values to by modifing X_ADDR and Y_ADDR. In order to write a test image to memory, on every clock cycle, we increment either X_ADDR or Y_ADDR and write a corresponding color based on the value of X_ADDR and Y_ADDR. We attempted a different color bar to write to memory as our test image:</p>
                                <p class="text-center"><img src="https://github.com/Blue9/ece3400-team20/blob/gh-pages/img/portfolio/Screen%20Shot%202018-11-30%20at%202.52.07%20PM.png?raw=true" alt="Color bar"></p>
                                <h1><a id="Team_FPGA_Downsampling_199"></a>Team FPGA Downsampling:</h1>
                                <p>We wrote our downsampler based upon the timing diagram in the OV7670 datasheet: pages (7-10) in</p>
                                <p><a href="http://web.mit.edu/6.111/www/f2016/tools/OV7670_2006.pdf">http://web.mit.edu/6.111/www/f2016/tools/OV7670_2006.pdf</a></p>
                                <p>After reading the timing diagrams we developed the following pseduo-coded downsampler solution:</p>
                                <pre><code>When PCL goes high: // trigger when PCL goes high
                                
                                    if(VSYNC is high):
                                        // new frame comes in
                                        Reset X_Address and Y_Address
                                        Reset pixel_data
                                        Set Write_Enable to 0
                                        Set Toggle to 0
                                    else if (HREF is on falling edge):
                                        // new row data comes in
                                        Reset X_Address
                                        Increment Y_Address
                                        Reset pixel_data
                                        Set Write_Enable to 0
                                        Set Toggle to 0
                                   else if (HREF is high and toggle is 0): # Reference Point 1
                                        Set pixel data based on first_input_byte
                                        Set Write_Enable to 0
                                   else if (HREF is high and toggle is 1): # Reference Point 2
                                        Set pixel data based on second_input_byte
                                        Set Write_Enable to 1 # Write to Memory
                                </code></pre>
                                <p>To clarify the pseudo-code, we introduce the toggle variable because color transmission occurs by sending two bytes. Depending on the color scheme utilized, we may obtain the green color data from one byte and the red/blue color data from another byte. We first utilized an RGB565 scheme and so obtained the R/G data upon the first byte and the G/B data upon the second byte. And so we set <code>pixel_data[7], pixel_data[6], pixel_data[5], pixel_data[4], pixel_data[3], pixel_data[2]</code> to <code>first_input_byte[7], first_input_byte[6], first_input_byte[5], first_input_byte[2], first_input_byte[1], first_input_byte[0]</code> respectively in the Reference Point 1 section. And we set <code>pixel_data[1], pixel_data[0]</code> to <code>second_input_byte[4], second_input_byte[3]</code> respectively in the Reference Point 2 section.</p>
                                <p>The reason for this ordering can be found on page 9 in the OV7670 manual. We take the three most significant bits from the red transmission, the three most significant bits from the green transmission, and the two most significant bits from the blue transmission.</p>
                                <h1><a id="Team_FPGA_Color_Bar_Test_235"></a>Team FPGA Color Bar Test:</h1>
                                <p>In order to test whether this downsampling was accurate, we needed to enable the Color Bar Test on the Arduino. We added the following registers to do so in the OV7670 sketch from Team Arduino:</p>
                                <pre><code>OV7670_write_register(0x12, 0x0E); // enable color bar testes
                                OV7670_write_register(0x42, 0x08)
                                </code></pre>
                                <p>We obtained the following result:</p>
                                <p class="text-center"><img src="https://github.com/Blue9/ece3400-team20/blob/gh-pages/img/portfolio/ColorBar.PNG?raw=true" alt="color bar test"></p>
                                <h1><a id="Team_FPGA_Color_Detection_Settings_248"></a>Team FPGA Color Detection Settings:</h1>
                                <p>After obtaining a color bar test, we attempted to work on obtaining image quality such that we would be able to detect shape colors. Initial image quality of the camera was very poor and so we replaced all of our wires. After that did not work well, we attempted to play around with the register settings and utilized RGB 444 instead of RGB 565 for our tranmission setting. We found the following register settings to work well:</p>
                                <pre><code>
                                OV7670_write_register(0x12, (read_register_value(0x12) | 0x80)); // Reset All Registers
                                OV7670_write_register(0x0C, (read_register_value(0x0C) | 0x08)); // Enable Scaling
                                OV7670_write_register(0x11, (read_register_value(0x11) | 0x40)); // Enable External Clock
                                OV7670_write_register(0x12, (read_register_value(0x12) | 0x0C)); // Camera Resolution
                                OV7670_write_register(0x8C,  (read_register_value(0x8C) | 0x2); // Enable RGB444
                                OV7670_write_register(0x40, (read_register_value(0x40) | 0xD0); // Set the bit for RGB and Enable a Bit for 444
                                OV7670_write_register(0x1E, (read_register_value(0x1E) | 0x30));  // Mirror Flip
                                OV7670_write_register(0x14, (read_register_value(0x14) | 0x01)); // Gain setting
                                </code></pre>
                                <p>Since we decided to change to RGB 444, we also needed to change how we would down-sample in Reference Point 1 and Reference Point 2. And so we changed the ordering of pixel_data to <code>{input_data_1[3], input_data_1[2], input_data_1[1], input_data_2[7], input_data_2[6], input_data_2[5], input_data_2[3], input_data_2[2]}</code>.</p>
                                <p>With this setting, we obtained better image quality but still had issues with differentiating blue and red colors. And so we decided to fully saturate red, remove green, and threshold blue by utilizing the following scheme on the most significant bits: <code>pixel_data = {input_data_1[3], input_data_1[3], input_data_1[3], 0, 0, 0, input_data_2[3] | input_data_2[2], input_data_2[3] | input_data_2[2]}</code>. And so if the most significant bit of the red data was high, then we fully saturate red. And if either of the two most significant bits of blue were high, then we fully saturate blue. We found this scheme to work well for color detection.</p>
                                <h1><a id="Team_FPGA_Color_Detection_Image_Processor_268"></a>Team FPGA Color Detection, Image Processor:</h1>
                                <p>In order to differentiate between red and blue treasures, we kept counts on the number of blue and red pixels and thresholded whether the frame was blue or red. The code looked as the following pseudo-code:</p>
                                <pre><code>blueCount = 0
<!--                         -->redCount = 0 
<!--                         -->On every rising clock edge:
<!--                         -->    if HREF is high:
<!--                         -->        if incoming pixel has blue values all high (00000011): 
<!--                         -->            increment blue count
<!--                         -->        if incoming pixel has red values all high (11100000)
<!--                         -->            increment red count
<!--                         -->    if VSYNC is on rising edge:
<!--                         -->        compare blueCount and redCount against threshold
<!--                         -->        set the LEDs according whichever is higher than threshold
<!--                         -->    if VSYNC is on falling edge:
<!--                         -->        reset blueCount
<!--                         -->        reset redCount
                                </code></pre>
                                <p>Note that in order to determine rising and falling edges, we kept track of the previous values and compared against the current value. We empirically tested for the threshold and found 70 to be a good threshold.</p>
                                <h1><a id="Color_Detection_Demonstration_291"></a>Color Detection, Demonstration:</h1>
                                <p>Utilizing the register settings and the image processor in the previous two sections, we were able to obtain that we were able to read pixel values as demonstrated in the following videos:</p>
                                <p>
                                        <p class="text-center"><iframe width="560" height="315" src="https://www.youtube.com/embed/X9Ipn2ejz8U" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe></p>
                                </p>
                        </div>
                        <a class="btn btn-primary btn-lg rounded-pill portfolio-modal-dismiss mt-4" href="#">
                                <i class="fa fa-close"></i> Close Project</a>
                    </div>
                </div>
            </div>
        </div>
    </div>
    <!-- Portfolio Modal 5 -->
    <div class="portfolio-modal mfp-hide" id="portfolio-modal-5">
        <div class="portfolio-modal-dialog bg-white">
            <a class="close-button d-none d-md-block portfolio-modal-dismiss" href="#">
                <i class="fa fa-3x fa-times"></i>
            </a>
            <div class="container text-center">
                <div class="row">
                    <div class="col-lg-10 mx-auto">
                        <h2 class="text-secondary text-uppercase mb-0">Milestone 1</h2>
                        <hr class="star-dark mb-5">
                        <div class="text-left">
                            <p class="text-center"><iframe width="560" height="315" src="https://www.youtube.com/embed/Rsmbcb27Gc8"
                                frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></p>
                            <p><strong><em>Lab Procedure:</em></strong></p>
                            <p><em>Objective:</em> The objective of this milestone is to be able to add sensors to the
                                robot as well as program the robot such that the robot will be able to line follow and
                                turn along a grid in a figure eight fashion.</p>
                            <p>The first challenge was being able to determine the placement of the line sensors to
                                optimize line following and turning, knowing whether we are following a line or have
                                reached an intersection. We decided to use two infrared sensors with spacing slightly
                                greater than the tape’s width placed in front of the robot approximately 5 mm above the
                                floor. This placement was chosen so that when the robot detects an intersection both
                                line sensors would read white and while the robot is going straight along a path, both
                                line sensors would read black. If any of the line sensors read white exclusively, i.e.
                                the left sensor reads white and the right sensor reads black or vice-versa, we know
                                that the robot is angled against the line and should adjust accordingly. This
                                adjustment forms the basis for our line-following procedure. We then calibrated the
                                threshold value of each sensor by reading the value on the analog pins along lighter
                                and darker regions of the grid and averaged accordingly.</p>
                            <p><strong><em>Loop Iteration States:</em></strong></p>
                            <p>To implement line following and turning, on each loop iteration, we checked the value of
                                the line sensors with respect to the threshold and classified/adjusted according to the
                                following four states:</p>
                            <p><strong>Left on Right off</strong>: If the left sensor reads white and the right sensor
                                reads black, then we know that the robot is angled against the line, i.e. it has turned
                                too far to the right while line following. As a result, we adjust accordingly by
                                stopping the right wheel (turn left) until both line sensors detect black. We then
                                continue both wheels full speed ahead from there. The following code demonstrates what
                                would happen if this conditional is met:</p>
                            <pre><code>
<!--                         -->adjust_right(0);
<!--                         -->while (sensor_on_white(RIGHT_COLOR_PIN) && !sensor_on_white(LEFT_COLOR_PIN)) {
<!--                         -->    // Wait until neither on white.
<!--                         -->}
<!--                         -->adjust_right(MAX_SPEED);
                            </code></pre>
                            <p><strong>Left off right on</strong>: If the right sensor reads white and the right sensor
                                reads black, then we know that the robot is angled against the line in the other
                                direction, i.e. it has turned too far to the left while line following. As a result, we
                                adjust accordingly by stopping the left wheel (turn right) until both line sensors
                                detect black. We then continue both wheels full speed ahead from there. The following
                                code demonstrates what would happen if this conditional is met:</p>
                            <pre><code>adjust_left(0);
<!--                         -->while (sensor_on_white(LEFT_COLOR_PIN) && !sensor_on_white(RIGHT_COLOR_PIN)) {
<!--                         -->    // Wait until neither on white.
<!--                         -->}
<!--                         -->adjust_left(MAX_SPEED);
<!--                         --></code></pre>
                            <p><strong>Both On</strong>: Based off of the sensor alignment we chose, as aforementioned,
                                if both sensors are on, we must be at an intersection. In order to complete a figure
                                eight in the configuration we chose, the robot must turn left four times and then turn
                                right four times. We kept a variable that kept track of which turn number we are on and
                                used the mod function to decide whether to turn left or right. For example, if the
                                counter variable records the 9th turn, the robot must be at an equivalent position as
                                for the 1st turn (9 % 8 = 1) and turn left. Once we determined which way to turn, we
                                called our turning functions accordingly. The following code demonstrates what would
                                happen if this conditional is met:</p>
                            <pre><code>
<!--                         -->// If both the right and left sensors are on white,
<!--                         -->// then we are at an intersection.
<!--                         -->if (figure_eight_step % 8 &gt; 3) {
<!--                         -->    Serial.println(&quot;Turning left&quot;);
<!--                         -->    turn_left();
<!--                         -->} else {
<!--                         -->    Serial.println(&quot;Turning right&quot;);
<!--                         -->    turn_right();
<!--                         -->}
<!--                         -->figure_eight_step = (figure_eight_step + 1) % 8;
                            </code></pre>
                            <p><strong>Both off</strong>: If both sensors are off, then the robot is aligned with the
                                tape. In this case, we continue forward and have both wheels go at their full speeds.
                                The following code demonstrates what would happen if this conditional is met:</p>
                            <pre><code>
<!--                         -->// Move straight.
<!--                         -->adjust_left(MAX_SPEED);
<!--                         -->adjust_right(MAX_SPEED);
                            </code></pre>
                            <p><strong>Turning Procedures</strong></p>
                            <p>As aforementioned, since the sensors are slightly farther apart then the line is wide,
                                when both sensors detect a line, we know that an intersection has been found. We
                                developed functions to turn in each direction: turn_left() and turn_right(). The
                                function turn_left() stops the left wheel, sets the right wheel to full speed, delays
                                for 900 msec, and restarts the left wheel. The function turn_right() does the same with
                                the wheels switched. We arrived on the timing of 900 msec through rigorous testing. If
                                the turn is not exact, our line following procedure will ensure the robot returns to
                                being centered on the line for increased robustness.</p>
                            <pre><code>
<!--                         -->/**
<!--                         --> * Turn the robot left 90 degrees by slowing down the left servo.
<!--                         --> */
<!--                         -->void turn_left() {
<!--                         -->    adjust_left(0);
<!--                         -->    adjust_right(1);
<!--                         -->    delay(900);
<!--                         -->    adjust_left(1);
<!--                         -->}
<!--                         -->/**
<!--                         --> * Turn the robot right 90 degrees by slowing down the right servo.
<!--                         --> */
<!--                         -->void turn_right() {
<!--                         -->    adjust_left(1);
<!--                         -->    adjust_right(0);
<!--                         -->    delay(900);
<!--                         -->    adjust_right(1);
<!--                         -->}
                            </code></pre>
                            <p><strong>Adjust Functions:</strong></p>
                            <p>In order to reduce the confusion that results from writing values on the servos from 0
                                to 180 where 0 refers to full speed ahead on one wheel and 0 refers to full speed
                                backwards on the other, we developed adjust_left and adjust_right functions that take
                                in values from -1 to 1 that map to the corresponding servos values. For example,
                                adjust_left(0) and adjust_right(0) both refer to stopping, adjust_left(1) and
                                adjust_right(1) refer to full speed ahead, and adjust_left(-1) and adjust_right(-1)
                                refer to full speed backwards. These functions rely on a map function we wrote that
                                utilizes the Servos range of 0 to 180 as the out_min and out_max for the left wheel,
                                and 180 to 0 as the out_min and out_max for the right wheel.</p>
                            <pre><code>
<!--                         -->/**
<!--                         --> * Scale the given value to a new range.
<!--                         --> * @param value_to_map The value to scale.
<!--                         --> * @param in_min The lower bound of the original range.
<!--                         --> * @param in_max The upper bound of the original range.
<!--                         --> * @param out_min The lower bound of the new range.
<!--                         --> * @param out_max The upper bound of the new range.
<!--                         --> * @return The scaled value.
<!--                         --> */
<!--                         -->double map(double value_to_map, double in_min, double in_max, double out_min, double out_max) {
<!--                         -->    return (value_to_map - in_min) * (out_max - out_min) / (in_max - in_min) + out_min;
<!--                         -->}
                            </code></pre>
                        </div>
                        <a class="btn btn-primary btn-lg rounded-pill portfolio-modal-dismiss" href="#">
                            <i class="fa fa-close"></i> Close Project</a>
                    </div>
                </div>
            </div>
        </div>
    </div>
    <!-- Portfolio Modal 6 -->
    <div class="portfolio-modal mfp-hide" id="portfolio-modal-6">
        <div class="portfolio-modal-dialog bg-white">
            <a class="close-button d-none d-md-block portfolio-modal-dismiss" href="#">
                <i class="fa fa-3x fa-times"></i>
            </a>
            <div class="container text-center">
                <div class="row">
                    <div class="col-lg-10 text-left mx-auto">
                        <h2 class="text-secondary text-center text-uppercase mb-0">Milestone 2</h2>
                        <hr class="star-dark mb-5">
                        <h3 class="text-center"><a id="Maze_exploration_using_Right_Hand_Rule_2"></a>Maze exploration
                            using Right Hand Rule</h3>
                        <p>To add maze exploration functionality using the right hand rule, we incorporated the
                            following logic:</p>
                        <pre><code>
<!--                 -->if no_right_wall:
<!--                 -->      turn_right()
<!--                 -->  else if no_front_wall: // right wall is detected but no front wall is detected
<!--                 -->      go_straight()
<!--                 -->  else:
<!--                 -->      turn_left() // right wall and front wall is detected 
<!--                 --></code></pre>
                        <p>We try to turn right at every intersection, and if the right side is blocked, we try moving
                            forward. We added two short distance sensors for wall detection on the right and front of
                            the robot in addition to the IR sensor explored in Lab 2. We also added two indicator LEDs
                            to show whether the robot detects a front or right wall. Front and right wall detection
                            uses an experimentally determined threshold value from the distance sensors. We found 100
                            to be a good threshold for the front wall sensor and 200 to be a good threshold for the
                            right sensor. In case the robot reaches a dead end, the robot will initially start turning
                            left (evaluate the else statement) and detect that there is a wall in front of it. It will
                            then reverse the turn and turn 180 degrees to continue to maze.</p>
                        <p>Due to the extra amount of complexity that arose from wall detection and in order to make
                            our line following system more robust, we incorporated a finite state machine design for
                            navigation. We start our robot in a “move forward” state that iteratively checks our left
                            and right line sensors. If any of those sensors detect that we are no longer line
                            following, then we branch into their respective states to adjust accordingly. If any of the
                            adjusting or “move forward” states also detect that we are at an intersection, by checking
                            if both the left and right sensors are on white, and need to turn according to the right
                            hand rule logic above, we branch into a state that moves the robot slightly past the
                            intersection (forward_until_past_intersection) and sets the turn diction by setting a
                            variable called turn_direction. If we do need to turn, we branch into a state that performs
                            the turn based on the turn_direction (start_turn), determine that the turn is completed
                            (wait_until_turn_completed) based on the line sensors, and then branches back into the
                            “move forward” state and continue our maze detection.</p>
                        <p>Excerpts from the code are as follows. The “only_left_on_white” and “only_right_on_white”
                            function check if only one of the line sensors is on white based on empirical thresholds
                            from the previous lab. We also created “left_on_white” and “right_on_white” to check if the
                            respective sensor is on white. The “both_on_white” function checks if both the line sensors
                            are on white. The “turn_status[turn_direction]” returns “left_on_white” for a left turn and
                            “right_on_white” for a right turn as we start turning left when the left line sensor is no
                            longer on white and turn right when the right line sensor is no longer on white.</p>
                        <!--                 -->
                        <pre><code>int move_forward() {
<!--                 -->  Serial.println(&quot;move forward&quot;);
<!--                 -->  front_led_off(); // initialize the leds to not detect a wall
<!--                 -->  right_led_off();
<!--                 -->  set_left(1); // move the robot forward
<!--                 -->  set_right(1);
<!--                 -->  if (only_left_on_white()) return ADJUST_LEFT; // branch into adjust left state
<!--                 -->  if (only_right_on_white()) return ADJUST_RIGHT; // branch into adjust right state 
<!--                 -->  if (both_on_white() &amp;&amp; (front_wall() || !right_wall() || opticalFFT())) {
<!--                 -->  // check if we are at an intersection and if there is a front wall, no right wall, or a decoy robot
<!--                 -->  // in which case we need to turn
<!--                 -->    return FORWARD_UNTIL_PAST_INTERSECTION; // at an intersection and need to turn
<!--                 -->  }
<!--                 -->  return MOVE_FORWARD;
<!--                 -->}
<!--                 -->
<!--                 -->int forward_until_past_intersection() { // must turn after this state
<!--                 -->  Serial.println(&quot;forward_until_past_intersection&quot;);
<!--                 -->  set_left(1); // move forward
<!--                 -->  set_right(1);
<!--                 -->  if (both_on_white()) return FORWARD_UNTIL_PAST_INTERSECTION; // move a little past the intersection
<!--                 -->  if (!right_wall()) turn_direction = 1; // turn right if no right wall
<!--                 -->  else turn_direction = 0; // else turn left
<!--                 -->  if (!turn_status[turn_direction]()) {
<!--                 -->    // turn_status[turn_direction() checks if left or right line sensor is on white
<!--                 -->    // and so we turn when the respective sensor is past the intersection. We stop, delay, and turn.
<!--                 -->    set_left(0); 
<!--                 -->    set_right(0);
<!--                 -->    delay(10);
<!--                 -->    return START_TURN;
<!--                 -->  }
<!--                 -->  return FORWARD_UNTIL_PAST_INTERSECTION;
<!--                 -->}
<!--                 -->
<!--                 -->int start_turn() {
<!--                 -->  Serial.println(&quot;start turn&quot;);
<!--                 -->  set_left(turn_direction); // set the left and white wheels based on the turn direction
<!--                 -->  set_right(1 - turn_direction); 
<!--                 -->  if (!turn_status[turn_direction]()) return START_TURN;
<!--                 -->  if (turn_status[turn_direction]()) {
<!--                 -->    // if we have seen the respective sensor on white thus halfway done with turn
<!--                 -->    // But before completing turn, check for a dead end.
<!--                 -->    Serial.println(front_wall());
<!--                 -->    if (front_wall() &amp;&amp; turn_direction == 0) return UNDO_TURN;
<!--                 -->    else return WAIT_UNTIL_TURN_END;
<!--                 -->  }
<!--                 -->  return START_TURN;
<!--                 -->}
<!--                 -->
<!--                 -->int wait_until_turn_end() {
<!--                 -->  Serial.println(&quot;wait_until_turn_end&quot;);
<!--                 -->  set_left(turn_direction); 
<!--                 -->  set_right(1 - turn_direction);
<!--                 -->  // turn_status[turn_direction] checks if either the left or right line sensor is on white.
<!--                 -->  // we continue turning until respective sensor is no longer on white line
<!--                 -->  if (turn_status[turn_direction]()) return WAIT_UNTIL_TURN_END;
<!--                 -->  if (!turn_status[turn_direction]()) return MOVE_FORWARD;
<!--                 -->  return WAIT_UNTIL_TURN_END;
<!--                 -->}
<!--                 -->
<!--                 -->
<!--                 --></code></pre>
                        <!--                 -->
                        <p>Once we developed these states and branching conditions, wall-detection worked. The
                            following video demonstrates how our robot is able to detect walls and line follows. Note
                            the flashing LEDs.</p>
                        <h1><p class="text-center"><iframe width="560" height="315" src="https://www.youtube.com/embed/sdR-A3kpq3s"
                                frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></p></h1>
                        <h3><a id="Avoiding_other_Robots_88"></a>Avoiding other Robots</h3>
                        <p>In order to avoid other robots, we attached an IR Sensor to the front of the robot. The
                            robot performs an FFT utilizing the “optical_FFT” function written in Lab 2. If we do
                            detect a decoy robot, then we treat it as a wall in our wall detection algorithm above.
                            This is why we added an OR statement with “optical_FFT()” above in the “move_forward” state
                            as well as added similar conditions in the “adjust_left” and “adjust_right” states. We
                            initially had issues with integrating the FFT but we adjusted our value of ADCSRA and we
                            found it to work.</p>
                        <p>A video of our robot avoiding other robots, walls, and line following is as follows:</p>
                        <h1><p class="text-center"><iframe width="560" height="315" src="https://www.youtube.com/embed/XYCFWvGQjow"
                                frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></p></h1>

                        </div>
                </div>
                <a class="btn btn-primary btn-lg rounded-pill portfolio-modal-dismiss mt-4" href="#">
                        <i class="fa fa-close"></i> Close Project</a>
            </div>
        </div>
    </div>

    <!-- Portfolio Modal 7 -->
    <div class="portfolio-modal mfp-hide" id="portfolio-modal-7">
            <div class="portfolio-modal-dialog bg-white">
                    <a class="close-button d-none d-md-block portfolio-modal-dismiss" href="#">
                        <i class="fa fa-3x fa-times"></i>
                    </a>
                    <div class="container text-center">
                        <div class="row">
                            <div class="col-lg-10 text-left mx-auto">
                                <h2 class="text-secondary text-center text-uppercase mb-0">Milestone 3</h2>
                                <hr class="star-dark mb-5">
                                <h3><a id="Maze_Exploration_using_Depth_First_Search_2"></a>Maze Exploration using Depth First Search</h3>
                                <p>To be able to map the entire maze, a right hand wall traversal is not sufficient, as it will not be able to reach isolated elements in the body of the maze. Instead, our robot will implement a depth first search (DFS) to map the entire maze.</p>
                                <h3><a id="Depth_First_Search_Algorithm_6"></a>Depth First Search Algorithm</h3>
                                <p>To begin we have an empty stack to signify the nodes which on our “frontier”. The frontier is all of the intersections in which we have been adjacent to and could have moved too, but have not yet visited. At each intersection we add all of adjacent nodes that do not have a wall inbetween to the fronteir stack with the condition that we have not yet visited said node. This can be checked by looking at the 2D array we have to map the maze from <strong>Lab 3</strong>.</p>
                                <p>The image below shows how the DFS tree is created and the corresponding frontier stack:</p>
                                <p style="width:100%" class="text-center"><img src="https://raw.githubusercontent.com/Blue9/ece3400-team20/gh-pages/img/portfolio/DFS_tree.PNG" alt="DFS Tree" title="DFS Tree"><br>
                                <img src="https://raw.githubusercontent.com/Blue9/ece3400-team20/gh-pages/img/portfolio/DFS_frontier.PNG" alt="DFS Frontier" title="DFS Frontier"></p>
                                <p>To choose which node to navigate to the top element of the frontier stack is popped giving the coordinates of the closest unexplored node in the DFS tree. For the majority of exploration this node will be adjacent to the current so we can simply turn or continue straight. If the node is not adjacent, we must backtrack untill it is adjacent, requiring another data structure: a stack of previous moves. This stack will add the past move to the stack so we can retrace our path to find the next node in the frontier set. The robot will continue to retrace until the next node is adjacent when it will move to the node and then repeat the process. We also ensure that when backtracking the moves are not added to the previous moves stack to avoid oscillations in movement.</p>
                                <p>When elements are pushed to the frontier stack, if available, the node which is directly in front of the robot will be placed on the top of the stack to prioritize forward movement over turning. This is becuase turning at an intersection takes significantly more time than moving straight and thus should be avoided as given the option.</p>
                                <p>The order nodes are pushed to the stack is LEFT, RIGHT, FRONT, so the following would be the mapping of a 4x5 maze with a single wall:</p>
                                <p style="width:100%" class="text-center"><img src="https://raw.githubusercontent.com/Blue9/ece3400-team20/gh-pages/img/portfolio/DFS_search.PNG" alt="DFS Map" title="DFS Map"></p>
                                <p><em>NOTE: The above algorithm and graphics were taken from ECE 3400 LEC16 - 2018</em></p>
                                <h3><a id="Algorithm_Integration_25"></a>Algorithm Integration</h3>
                                <p>To incorporate DFS, two additional data structures were needed as described above: <code>StackArray &lt;Coord&gt; dfs_frontier</code> and <code>StackArray &lt;Coord&gt; dfs_prev_moves</code> utilizing the <code>StackArray</code> library. They are of type <code>Coord</code> which represent coordinates in the maze, defined as follows:</p>
                                <pre><code class="language-C"><span class="hljs-keyword">class</span> Coord{
                                  <span class="hljs-keyword">public</span>:
                                    byte x, y;
                                    Coord(byte x_coord, byte y_coord){ 
                                      x = x_coord; y = y_coord;
                                    } 
                                };
                                </code></pre>
                                <p>At each intersection, the robot first updates its position, adding the current node to <code>dfs_prev_moves</code>, and adds all of the adjacent nodes to <code>dfs_frontier</code> if their is not a wall between them. Next, the we peek at <code>dfs_frontier</code> and check if it is adjacent to the current node and there is no wall in between. If it is adjacent, the element is popped, the global direction (N, S, E, W) is found by comparing the two <code>Coord</code> x and y values, which is then translated to a turn direction (<code>left</code>, <code>right</code>, <code>straight</code>, <code>turn around</code>) to give the robot direction. If it is not adjacent, the top of <code>dfs_prev_moves</code> is popped to begin backtracking towards the next element in the frontier set. The turn direction is calculated in the exact same way as if the node was adjacent.</p>
                                <pre><code>handle_intersection():
                                  update_static_array // update internally wall locations at current point
                                  if( ! check_neighbors(current_location, dfs_next.nextMove() ):
                                    move_temp = dfs_next.pop()
                                    next_direction = get_direction(move_temp, current_location, current_bearing) // obtain NSEW direction
                                  else:
                                    dfs_prev_moves.push(current_location)
                                    move_temp = dfs_next.pop()
                                    next_direction = get_direction(move_temp, current_location, current_bearing) // obtain NSEW direction
                                  next_move = get_next_move(current_location, next_direction)
                                  return next_move
                                </code></pre>
                                <p>The above pseudocode is a summary of our implementation.</p>
                                <h3><a id="Demonstration_56"></a>Demonstration</h3>
                                <p>Below is a short maze traversal.  This shows that backtracking works as well as general DFS:<br>
                                <p class="text-center"><iframe width="560" height="315" src="https://www.youtube.com/embed/QDcZ0XjWdXo"
                                frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></p>
                                Below is a long maze traversal.  This demonstrates both backtracking, escaping infinite loops and some edge cases:<br>
                                <p class="text-center"><iframe width="560" height="315" src="https://www.youtube.com/embed/M_wKZIesAfc"
                                frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></p>
                                <h3><a id="Testing_62"></a>Testing</h3>
                                <p>We initially ran into problems due to incorrect bearings.  After fixing this problem, we ran into problems with storing data in arrays.  We then fixed this problem pretty easily.<br>
                                The main problem we are still having is the fact that the sensor thresholds change with ambient light.  Furthermore, if the robot goes to a square and detects a wall when there isn’t any, it will not add that adjacent square to the DFS stack.  This is sometimes corrected if the robot travels to an adjacent square of that.  We decreased this occurance by changing thresholds appropriately.</p>
                                <p>We then enabled GUI communication.  We could not get a signal however and ran out of time.</p>
                                 
                                 
                                 
                                 
                            </div>
                        </div>
                        <a class="btn btn-primary btn-lg rounded-pill portfolio-modal-dismiss mt-4" href="#">
                            <i class="fa fa-close"></i> Close Project</a>
                    </div>
                        
            </div>
    </div>

    <!-- Portfolio Modal 8 -->
    <div class="portfolio-modal mfp-hide" id="portfolio-modal-8">
            <div class="portfolio-modal-dialog bg-white">
                    <a class="close-button d-none d-md-block portfolio-modal-dismiss" href="#">
                        <i class="fa fa-3x fa-times"></i>
                    </a>
                    <div class="container text-center">
                        <div class="row">
                            <div class="col-lg-10 text-left mx-auto">
                                <h2 class="text-secondary text-center text-uppercase mb-0">Milestone 4</h2>
                                <hr class="star-dark mb-5">
                                <h3><a id="Distinguishing_between_Red_and_Blue_Treasures_0"></a>Distinguishing between Red and Blue Treasures</h3>
                                <p>In order to be able to distinguish between red and blue treasures, we had made our camera output to be RGB444 and saturated the down-sampling for red and thresholded the blue values coming in. When designing our downsampler and image processor for Lab 4, we had the goal of distinguishing red and blue treasures in mind. This design is what allowed us to observe this outcome in the following video in which we place red and blue treasures in front of our camera and successfully distinguish between them:</p>
                                <p class="text-center"><iframe width="560" height="315" src="https://www.youtube.com/embed/X9Ipn2ejz8U" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe></p>
                                <h1><a id="Shape_Detection_6"></a>Shape Detection</h1>
                                <p>Once we were able to distinguish between a red and a blue treasure, the next step was to determine what shape a treasure is. Our strategy was to sample three equally distanced pixel rows from the image. We noticed the following:</p>
                                <p class="text-center"><img src="https://github.com/Blue9/ece3400-team20/blob/gh-pages/img/portfolio/Screen%20Shot%202018-11-30%20at%207.21.46%20PM.png?raw=true" alt="shape_detection_algo"></p>
                                <p>As demonstrated by the figure, if we compare the intensity of red between the three samples, we would be able to determine which shape it is. We could even average these samples for greater accuracy. The rules would be as follows:</p>
                                <pre><code>Take Color Samples at Rows 58, 72, and 94.

                                If color intensity is same across all samples:
                                shape = rectanlge
                                
                                If color intensity is highest at middle row and edge rows are smaller:
                                shape = diamond

                                If color intensity is monotonically increasing/decreasing:
                                shape = triangle
                                </code></pre>
                                <p>With the amount of noise that we were experiencing with the camera and after hours of changing registers and wires, we were unable to achieve a steady feed that could implement the strategy above. However, the following video demonstrates what should happen but was not what we able to get:</p>
                                <p class="text-center"><iframe width="560" height="315" src="https://www.youtube.com/embed/hRiXi_E4cNg" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe></p>
                                <p>After a lot of experimentation, we hypothesized that our VSYNC line was not working properly. We believed this because our image would constantly reset to (0, 0) before writing a full image. The only way both coordinates can be reset is if VSYNC is pulled high, so the VSYNC line was likely experiencing some noise and getting pulled up when it shouldn’t have.</p>
                                <p>To fix this issue, we first got rid of VSYNC resets altogether and instead reset coordinates when Y_ADDR went off the screen. This worked to an extent, but oftentimes our screen would be offset and wrap around vertically. To fix this, we decided to use VSYNC but only every 44 million cycles. This prevented the screen from becoming offset and also prevented the unusual coordinate resets we were experiencing.</p>
                                <p>After doing this, we saturated all pixels to only have full red, blue, or green intensity. This finally gave us a stable image and we were able to conduct shape detection, shown in the video below. If the 3 left bits are high, then we detect a triangle. If the middle 3 bits are high, we’ve detected a square. If the rightmost 2 bits are high, we’ve detected a diamond. Otherwise, we do not detect a treasure and all bits are high.</p>
                                <p class="text-center"><iframe width="560" height="315" src="https://www.youtube.com/embed/FeldkNRAHkY" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe></p>
                                <p>As seen in the video, our output is very stable, and we are not completely sure why this is the case, considering the video feed is quite noisy. This is likely due to our thresholds being calibrated far apart enough to differentiate the shapes well enough. However, we found that running this test multiple times and with different shape orientations we were unable to detect the shape. Because of that, we feel it is best to omit this from our robot.</p>

                            </div>
                        </div>
                        <a class="btn btn-primary btn-lg rounded-pill portfolio-modal-dismiss mt-4" href="#">
                                <i class="fa fa-close"></i> Close Project</a>
                    </div>
        </div>
    </div>
    <!-- Bootstrap core JavaScript -->
    <script src="vendor/jquery/jquery.min.js"></script>
    <script src="vendor/bootstrap/js/bootstrap.bundle.min.js"></script>
    <!-- Plugin JavaScript -->
    <script src="vendor/jquery-easing/jquery.easing.min.js"></script>
    <script src="vendor/magnific-popup/jquery.magnific-popup.min.js"></script>
    <!-- Contact Form JavaScript -->
    <script src="js/jqBootstrapValidation.js"></script>
    <script src="js/contact_me.js"></script>
    <!-- Custom scripts for this template -->
    <script src="js/freelancer.js"></script>
</body>

</html>
